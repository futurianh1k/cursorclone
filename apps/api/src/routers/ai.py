"""
AI ë¼ìš°í„°
- POST /api/ai/explain   - ì½”ë“œ ì„¤ëª…
- POST /api/ai/chat      - ëŒ€í™”í˜• ì±„íŒ…
- POST /api/ai/rewrite   - ì½”ë“œ ìˆ˜ì •
- POST /api/ai/plan      - ì‘ì—… ê³„íš ìˆ˜ë¦½
- POST /api/ai/agent     - ìë™ ì½”ë“œ ì‘ì„±/ìˆ˜ì •
- POST /api/ai/debug     - ë²„ê·¸ ë¶„ì„/ìˆ˜ì •

ì„¤ì •ì€ config.pyì—ì„œ ì¤‘ì•™ ê´€ë¦¬ë©ë‹ˆë‹¤.
"""

from fastapi import APIRouter, HTTPException, status
from ..config import settings
from ..models import (
    AIExplainRequest,
    AIExplainResponse,
    AIChatRequest,
    AIChatResponse,
    AIRewriteRequest,
    AIRewriteResponse,
    # AI Modes
    AIMode,
    AIPlanRequest,
    AIPlanResponse,
    TaskStep,
    AIAgentRequest,
    AIAgentResponse,
    FileChange,
    AIDebugRequest,
    AIDebugResponse,
    BugFix,
    ErrorResponse,
)
from ..context_builder import (
    DefaultContextBuilder,
    ContextBuildRequest,
    ContextSource,
    ContextSourceType,
    ActionType,
    SecurityError,
)
from ..llm import (
    get_llm_client,
    LLMError,
    LLMTimeoutError,
)
from ..utils.filesystem import get_workspace_root, workspace_exists

router = APIRouter(prefix="/api/ai", tags=["ai"])

# Context Builder ì¸ìŠ¤í„´ìŠ¤ (ì „ì—­ ë˜ëŠ” ì˜ì¡´ì„± ì£¼ì…)
_context_builder_cache = {}


def _get_context_builder(workspace_id: str) -> DefaultContextBuilder:
    """
    ì›Œí¬ìŠ¤í˜ì´ìŠ¤ë³„ Context Builder ê°€ì ¸ì˜¤ê¸°
    
    ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë£¨íŠ¸ ê²½ë¡œëŠ” utils.filesystem.get_workspace_root()ì—ì„œ ê°€ì ¸ì˜´
    """
    if workspace_id not in _context_builder_cache:
        workspace_root = get_workspace_root(workspace_id)
        
        if not workspace_exists(workspace_root):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": f"Workspace not found: {workspace_id}", "code": "NOT_FOUND"},
            )
        
        _context_builder_cache[workspace_id] = DefaultContextBuilder(
            workspace_root=str(workspace_root),
        )
    return _context_builder_cache[workspace_id]


def _validate_workspace_access(ws_id: str, user_id: str = None) -> bool:
    """
    ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì ‘ê·¼ ê¶Œí•œ ê²€ì¦
    
    ê²€ì¦ ë¡œì§:
    1. ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    2. ì‚¬ìš©ìê°€ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì†Œìœ ìì¸ì§€ í™•ì¸ (user_id ì œê³µ ì‹œ)
    3. ê³µìœ  ê¶Œí•œ í™•ì¸ (ì¶”í›„ êµ¬í˜„)
    
    Args:
        ws_id: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ID
        user_id: ì‚¬ìš©ì ID (ì„ íƒì )
    
    Returns:
        ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€
    """
    from ..utils.filesystem import get_workspace_root, workspace_exists
    
    # 1. ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    workspace_root = get_workspace_root(ws_id)
    if not workspace_exists(workspace_root):
        return False
    
    # 2. ì‚¬ìš©ì ê¶Œí•œ í™•ì¸ (user_id ì œê³µ ì‹œ)
    if user_id:
        # ì›Œí¬ìŠ¤í˜ì´ìŠ¤ IDì—ì„œ ì†Œìœ ì ì¶”ì¶œ (í˜•ì‹: owner_id/workspace_name ë˜ëŠ” ws_name)
        if "/" in ws_id:
            owner_id = ws_id.split("/")[0]
            if owner_id != user_id:
                # ê³µìœ  ê¶Œí•œ í™•ì¸ (DB ì¡°íšŒ - ì¶”í›„ êµ¬í˜„)
                # í˜„ì¬ëŠ” ì†Œìœ ìê°€ ì•„ë‹ˆì–´ë„ ì¡´ì¬í•˜ë©´ ì ‘ê·¼ í—ˆìš© (PoC)
                pass
    
    return True


def _validate_path(path: str) -> bool:
    """ê²½ë¡œ ê²€ì¦ (íƒˆì¶œ ë°©ì§€)"""
    if ".." in path:
        return False
    if path.startswith("/"):
        return False
    return True


async def _read_file_content(workspace_id: str, file_path: str) -> str:
    """
    íŒŒì¼ ë‚´ìš© ì½ê¸° (files ë¼ìš°í„°ì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©)
    """
    import os
    from ..utils.filesystem import get_workspace_root, validate_path, read_file_content, workspace_exists
    
    workspace_root = get_workspace_root(workspace_id)
    
    # ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not workspace_exists(workspace_root):
        raise ValueError(f"Workspace not found: {workspace_id}")
    
    # ê²½ë¡œ ê²€ì¦ ë° íŒŒì¼ ì½ê¸°
    try:
        full_path = validate_path(file_path, workspace_root)
        content, _ = read_file_content(full_path)
        return content
    except (ValueError, FileNotFoundError) as e:
        raise ValueError(f"Failed to read file: {e}")


@router.post(
    "/explain",
    response_model=AIExplainResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        401: {"model": ErrorResponse, "description": "Unauthorized"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        404: {"model": ErrorResponse, "description": "File not found"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ì½”ë“œ ì„¤ëª…",
    description="ì„ íƒëœ ì½”ë“œì— ëŒ€í•œ AI ì„¤ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
)
async def explain_code(request: AIExplainRequest):
    """
    ì„ íƒëœ ì½”ë“œì— ëŒ€í•œ AI ì„¤ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    íë¦„: API â†’ Context Builder â†’ vLLM â†’ ì‘ë‹µ
    
    âš ï¸ ì£¼ì˜ (AGENTS.md ê·œì¹™)
    - APIëŠ” LLMì„ ì§ì ‘ í˜¸ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
    - ë°˜ë“œì‹œ Context Builderë¥¼ ê²½ìœ í•´ì•¼ í•©ë‹ˆë‹¤
    """
    # ê²½ë¡œ ê²€ì¦
    if not _validate_path(request.file_path):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Invalid path", "code": "AI_INVALID_PATH"},
        )
    
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    try:
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        file_content = await _read_file_content(request.workspace_id, request.file_path)
        
        # Context Builder ì¤€ë¹„
        context_builder = _get_context_builder(request.workspace_id)
        
        # ì»¨í…ìŠ¤íŠ¸ ì†ŒìŠ¤ êµ¬ì„±
        sources = [
            ContextSource(
                type=ContextSourceType.FILE if not request.selection else ContextSourceType.SELECTION,
                path=request.file_path,
                content=file_content,
                range=request.selection,
            )
        ]
        
        # Context Builder í˜¸ì¶œ
        context_request = ContextBuildRequest(
            workspace_id=request.workspace_id,
            action=ActionType.EXPLAIN,
            instruction=f"ë‹¤ìŒ ì½”ë“œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”: {request.file_path}",
            sources=sources,
        )
        
        context_response = await context_builder.build(context_request)
        
        # vLLM í˜¸ì¶œ (ê°œë°œ ëª¨ë“œì—ì„œëŠ” Mock ì‘ë‹µ)
        dev_mode = settings.DEV_MODE
        
        try:
            if dev_mode:
                # ê°œë°œ ëª¨ë“œ: Mock ì‘ë‹µ ìƒì„±
                # ì„ íƒëœ ì½”ë“œ ì¶”ì¶œ
                if request.selection:
                    lines = file_content.split("\n")
                    start = request.selection.start_line - 1
                    end = request.selection.end_line
                    selected_code = "\n".join(lines[start:end])
                else:
                    selected_code = file_content[:500] + ("..." if len(file_content) > 500 else "")
                
                explanation = f"""## ì½”ë“œ ë¶„ì„ (ê°œë°œ ëª¨ë“œ)

**íŒŒì¼**: `{request.file_path}`

### ì½”ë“œ ë‚´ìš©
```
{selected_code}
```

### ì„¤ëª…
ì´ ì½”ë“œëŠ” `{request.file_path}` íŒŒì¼ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.

> âš ï¸ **ê°œë°œ ëª¨ë“œ**: ì‹¤ì œ LLM ì„œë¹„ìŠ¤ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
> vLLM ì„œë²„ë¥¼ ì‹œì‘í•˜ë©´ ì‹¤ì œ AI ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**vLLM ì„¤ì • ë°©ë²•**:
```bash
# docker-compose.ymlì— vLLM ì„œë¹„ìŠ¤ ì¶”ê°€ ë˜ëŠ”
# VLLM_BASE_URL í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export VLLM_BASE_URL=http://your-vllm-server:8000/v1
```
"""
                return AIExplainResponse(
                    explanation=explanation,
                    tokensUsed=0,
                )
            
            # í”„ë¡œë•ì…˜ ëª¨ë“œ: ì‹¤ì œ LLM í˜¸ì¶œ
            llm_client = get_llm_client()
            
            # ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
            messages = [
                {"role": msg.role, "content": msg.content}
                for msg in context_response.messages
            ]
            
            llm_response = await llm_client.chat(
                messages=messages,
                max_tokens=request.max_tokens if hasattr(request, "max_tokens") else None,
            )
            
            # ì‘ë‹µ ì¶”ì¶œ
            explanation = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            if not explanation:
                explanation = "[LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤]"
            
            return AIExplainResponse(
                explanation=explanation,
                tokensUsed=tokens_used,
            )
            
        except LLMTimeoutError as e:
            raise HTTPException(
                status_code=status.HTTP_504_GATEWAY_TIMEOUT,
                detail={"error": "LLM timeout", "code": "LLM_TIMEOUT", "detail": str(e)},
            )
        except LLMError as e:
            # LLM ì—ëŸ¬ ì‹œ ê°œë°œ ëª¨ë“œ ì‘ë‹µ ì œê³µ
            explanation = f"""## LLM ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨

**ì—ëŸ¬**: {str(e)}

vLLM ì„œë²„ê°€ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•Šê±°ë‚˜ ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**í•´ê²° ë°©ë²•**:
1. vLLM ì„œë²„ ìƒíƒœ í™•ì¸
2. `VLLM_BASE_URL` í™˜ê²½ë³€ìˆ˜ í™•ì¸
3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
"""
            return AIExplainResponse(
                explanation=explanation,
                tokensUsed=0,
            )
        
    except SecurityError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Security validation failed", "code": "SECURITY_ERROR", "detail": str(e)},
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR"},
        )


@router.post(
    "/chat",
    response_model=AIChatResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        401: {"model": ErrorResponse, "description": "Unauthorized"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="AI ì±„íŒ…",
    description="ì½”ë“œì— ëŒ€í•œ ì§ˆë¬¸, ìˆ˜ì • ìš”ì²­, ì£¼ì„ ì¶”ê°€ ë“± ììœ ë¡œìš´ ëŒ€í™”í˜• AI ì¸í„°í˜ì´ìŠ¤.",
)
async def chat_with_ai(request: AIChatRequest):
    """
    ëŒ€í™”í˜• AI ì¸í„°í˜ì´ìŠ¤.
    
    - ì‚¬ìš©ì ì§ˆë¬¸ + íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ LLMì— ì „ë‹¬
    - "ì£¼ì„ ë‹¬ì•„ì¤˜", "ì´ ì½”ë“œ ë­ì•¼?", "ë²„ê·¸ ì°¾ì•„ì¤˜" ë“± ììœ ë¡œìš´ ìš”ì²­ ì²˜ë¦¬
    - íŒŒì¼ì´ ì—†ì–´ë„ ì¼ë°˜ ì§ˆë¬¸ ê°€ëŠ¥
    
    íë¦„: API â†’ Context Builder â†’ vLLM â†’ ì‘ë‹µ
    """
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    # íŒŒì¼ ê²½ë¡œ ê²€ì¦
    if request.file_path and not _validate_path(request.file_path):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Invalid path", "code": "AI_INVALID_PATH"},
        )
    
    try:
        dev_mode = settings.DEV_MODE
        
        # íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        file_content = None
        if request.file_path:
            try:
                if request.file_content:
                    file_content = request.file_content
                else:
                    file_content = await _read_file_content(request.workspace_id, request.file_path)
            except Exception as e:
                # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨í•´ë„ ì±„íŒ…ì€ ê³„ì†
                file_content = f"[íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}]"
        
        # ì„ íƒëœ ì½”ë“œ ì¶”ì¶œ
        selected_code = None
        if file_content and request.selection:
            lines = file_content.split("\n")
            start = request.selection.start_line - 1
            end = request.selection.end_line
            selected_code = "\n".join(lines[start:end])
        
        if dev_mode:
            # ê°œë°œ ëª¨ë“œ: Mock ì‘ë‹µ ìƒì„±
            response_text = _generate_mock_chat_response(
                message=request.message,
                file_path=request.file_path,
                file_content=file_content,
                selected_code=selected_code,
            )
            
            return AIChatResponse(
                response=response_text,
                tokensUsed=0,
                suggestedAction=_detect_action(request.message),
            )
        
        # í”„ë¡œë•ì…˜ ëª¨ë“œ: ì‹¤ì œ LLM í˜¸ì¶œ
        try:
            llm_client = get_llm_client()
            
            # íŒŒì¼ì´ ìˆìœ¼ë©´ Context Builder ì‚¬ìš©, ì—†ìœ¼ë©´ ì§ì ‘ í˜¸ì¶œ
            if file_content:
                context_builder = _get_context_builder(request.workspace_id)
                
                sources = [
                    ContextSource(
                        type=ContextSourceType.SELECTION if selected_code else ContextSourceType.FILE,
                        path=request.file_path,
                        content=file_content,
                        range=request.selection,
                    )
                ]
                
                context_request = ContextBuildRequest(
                    workspace_id=request.workspace_id,
                    action=ActionType.EXPLAIN,
                    instruction=request.message,
                    sources=sources,
                )
                
                context_response = await context_builder.build(context_request)
                messages = [
                    {"role": msg.role, "content": msg.content}
                    for msg in context_response.messages
                ]
            else:
                # íŒŒì¼ ì—†ì´ ì¼ë°˜ ì±„íŒ…
                messages = [
                    {"role": "system", "content": "You are a helpful coding assistant. Answer questions about programming, software development, and technology. Respond in the same language as the user's question."},
                    {"role": "user", "content": request.message},
                ]
            
            llm_response = await llm_client.chat(messages=messages)
            
            response_text = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            return AIChatResponse(
                response=response_text or "[ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤]",
                tokensUsed=tokens_used,
                suggestedAction=_detect_action(request.message),
            )
            
        except (LLMTimeoutError, LLMError) as e:
            # LLM ì—ëŸ¬ ì‹œ ì¹œí™”ì ì¸ ì‘ë‹µ
            return AIChatResponse(
                response=f"âš ï¸ LLM ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n**ì—ëŸ¬**: {str(e)}\n\n`VLLM_BASE_URL` í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
                tokensUsed=0,
            )
            
    except SecurityError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Security validation failed", "code": "SECURITY_ERROR", "detail": str(e)},
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR", "detail": str(e)},
        )


def _detect_action(message: str) -> str | None:
    """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ì œì•ˆ ì•¡ì…˜ ê°ì§€"""
    message_lower = message.lower()
    
    rewrite_keywords = ["ìˆ˜ì •", "ë³€ê²½", "ë°”ê¿”", "ì¶”ê°€", "ì‚­ì œ", "ì£¼ì„", "ë¦¬íŒ©í„°", "fix", "change", "add", "remove", "comment"]
    explain_keywords = ["ì„¤ëª…", "ë­ì•¼", "ë­”ê°€ìš”", "ì•Œë ¤ì¤˜", "ì–´ë–»ê²Œ", "explain", "what", "how", "why"]
    
    if any(kw in message_lower for kw in rewrite_keywords):
        return "rewrite"
    if any(kw in message_lower for kw in explain_keywords):
        return "explain"
    
    return None


def _generate_mock_chat_response(
    message: str,
    file_path: str | None,
    file_content: str | None,
    selected_code: str | None,
) -> str:
    """ê°œë°œ ëª¨ë“œìš© Mock ì‘ë‹µ ìƒì„±"""
    # ì‚¬ìš©ì ìš”ì²­ ë¶„ì„
    action = _detect_action(message)
    
    if not file_path:
        return f"""ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì½”ë“œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì§ˆë¬¸**: {message}

í˜„ì¬ íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—´ê³  ì½”ë“œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì‹œë©´ ë” ì •í™•í•œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---
> âš ï¸ **ê°œë°œ ëª¨ë“œ**: vLLM ì„œë²„ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
"""
    
    code_preview = selected_code or (file_content[:300] + "..." if file_content and len(file_content) > 300 else file_content)
    
    if action == "rewrite" and "ì£¼ì„" in message:
        # ì£¼ì„ ì¶”ê°€ ìš”ì²­ì— ëŒ€í•œ Mock ì‘ë‹µ
        return f"""## ì£¼ì„ ì¶”ê°€ ì œì•ˆ

**íŒŒì¼**: `{file_path}`

**ìš”ì²­**: {message}

### ì›ë³¸ ì½”ë“œ
```
{code_preview}
```

### ì œì•ˆëœ ë³€ê²½ì‚¬í•­

ì½”ë“œì— ì£¼ì„ì„ ì¶”ê°€í•˜ë ¤ë©´ **Rewrite ëª¨ë“œ**ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:
1. ì£¼ì„ì„ ë‹¬ ì½”ë“œ ì˜ì—­ì„ ì„ íƒ
2. "Rewrite" ëª¨ë“œë¡œ ì „í™˜
3. "ì£¼ì„ ë‹¬ì•„ì¤˜"ë¼ê³  ì…ë ¥

---
> âš ï¸ **ê°œë°œ ëª¨ë“œ**: ì‹¤ì œ ì½”ë“œ ìˆ˜ì •ì€ vLLM ì—°ê²° í›„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
> `VLLM_BASE_URL` í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
"""
    
    return f"""## AI ì‘ë‹µ

**íŒŒì¼**: `{file_path}`

**ì§ˆë¬¸**: {message}

### ì½”ë“œ ë‚´ìš©
```
{code_preview}
```

### ë¶„ì„

ì´ ì½”ë“œëŠ” `{file_path}` íŒŒì¼ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.

{f"ì„ íƒëœ ì˜ì—­ ({len(selected_code.split(chr(10)))}ì¤„)ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤." if selected_code else "ì „ì²´ íŒŒì¼ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤."}

---
> âš ï¸ **ê°œë°œ ëª¨ë“œ**: ì‹¤ì œ AI ë¶„ì„ì€ vLLM ì„œë²„ ì—°ê²° í›„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""


@router.post(
    "/rewrite",
    response_model=AIRewriteResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        401: {"model": ErrorResponse, "description": "Unauthorized"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        404: {"model": ErrorResponse, "description": "File not found"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ì½”ë“œ ë¦¬ë¼ì´íŠ¸",
    description="ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ê³  unified diffë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
)
async def rewrite_code(request: AIRewriteRequest):
    """
    ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ê³  unified diffë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    âš ï¸ ì¤‘ìš”: ì´ APIëŠ” diffë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì‹¤ì œ ì ìš©ì€ /patch/validate â†’ /patch/applyë¥¼ í†µí•´ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
    
    íë¦„: API â†’ Context Builder â†’ vLLM â†’ diff ë°˜í™˜
          â†’ (í´ë¼ì´ì–¸íŠ¸) â†’ /patch/validate â†’ /patch/apply
    
    âš ï¸ ì£¼ì˜ (AGENTS.md ê·œì¹™)
    - APIëŠ” LLMì„ ì§ì ‘ í˜¸ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
    - ì½”ë“œ ë³€ê²½ì€ ë°˜ë“œì‹œ Patch ê²½ë¡œë¡œ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤
    """
    target_file = request.target.get("file", "")
    
    # ê²½ë¡œ ê²€ì¦
    if not _validate_path(target_file):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Invalid path", "code": "AI_INVALID_PATH"},
        )
    
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    try:
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        file_content = await _read_file_content(request.workspace_id, target_file)
        
        # ì„ íƒ ë²”ìœ„ ì¶”ì¶œ
        selection_range = None
        if "selection" in request.target:
            from ..context_builder import SelectionRange
            sel = request.target["selection"]
            selection_range = SelectionRange(
                start_line=sel.get("startLine", sel.get("start_line", 1)),
                end_line=sel.get("endLine", sel.get("end_line", 1)),
            )
        
        # Context Builder ì¤€ë¹„
        context_builder = _get_context_builder(request.workspace_id)
        
        # ì»¨í…ìŠ¤íŠ¸ ì†ŒìŠ¤ êµ¬ì„±
        sources = [
            ContextSource(
                type=ContextSourceType.SELECTION if selection_range else ContextSourceType.FILE,
                path=target_file,
                content=file_content,
                range=selection_range,
            )
        ]
        
        # Context Builder í˜¸ì¶œ
        context_request = ContextBuildRequest(
            workspace_id=request.workspace_id,
            action=ActionType.REWRITE,
            instruction=request.instruction,
            sources=sources,
        )
        
        context_response = await context_builder.build(context_request)
        
        # vLLM í˜¸ì¶œ (rewrite í…œí”Œë¦¿ ì‚¬ìš©)
        try:
            llm_client = get_llm_client()
            
            # ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
            messages = [
                {"role": msg.role, "content": msg.content}
                for msg in context_response.messages
            ]
            
            llm_response = await llm_client.chat(
                messages=messages,
                max_tokens=request.max_tokens if hasattr(request, "max_tokens") else None,
            )
            
            # ì‘ë‹µ ì¶”ì¶œ (unified diff í˜•ì‹)
            diff = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            if not diff:
                # ë¹ˆ diffì¸ ê²½ìš° ê¸°ë³¸ í˜•ì‹ ë°˜í™˜
                diff = f"""--- a/{target_file}
+++ b/{target_file}
@@ -1,0 +1,0 @@
"""
            
            return AIRewriteResponse(
                diff=diff,
                tokensUsed=tokens_used,
            )
            
        except LLMTimeoutError as e:
            raise HTTPException(
                status_code=status.HTTP_504_GATEWAY_TIMEOUT,
                detail={"error": "LLM timeout", "code": "LLM_TIMEOUT", "detail": str(e)},
            )
        except LLMError as e:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail={"error": "LLM service error", "code": "LLM_ERROR", "detail": str(e)},
            )
        
    except SecurityError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Security validation failed", "code": "SECURITY_ERROR", "detail": str(e)},
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR"},
        )


# ============================================================
# AI Plan Mode - ì‘ì—… ê³„íš ìˆ˜ë¦½
# ============================================================

@router.post(
    "/plan",
    response_model=AIPlanResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ì‘ì—… ê³„íš ìˆ˜ë¦½ (Plan ëª¨ë“œ)",
    description="ëª©í‘œë¥¼ ë¶„ì„í•˜ê³  ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤.",
)
async def create_plan(request: AIPlanRequest):
    """
    ì‚¬ìš©ìê°€ ì œì‹œí•œ ëª©í‘œë¥¼ ë¶„ì„í•˜ê³ , ì´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ 
    ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    ì˜ˆì‹œ:
    - "ë¡œê·¸ì¸ ê¸°ëŠ¥ ì¶”ê°€"
    - "í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±"
    - "ì½”ë“œ ë¦¬íŒ©í† ë§"
    """
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    try:
        dev_mode = settings.DEV_MODE
        
        # ê´€ë ¨ íŒŒì¼ ë‚´ìš© ìˆ˜ì§‘
        file_contents = {}
        if request.file_paths:
            for fp in request.file_paths[:5]:  # ìµœëŒ€ 5ê°œ íŒŒì¼
                if _validate_path(fp):
                    try:
                        content = await _read_file_content(request.workspace_id, fp)
                        file_contents[fp] = content
                    except:
                        pass
        
        if dev_mode:
            # ê°œë°œ ëª¨ë“œ: Mock ê³„íš ìƒì„±
            steps = _generate_mock_plan_steps(request.goal, file_contents)
            return AIPlanResponse(
                summary=f"'{request.goal}'ë¥¼ ìœ„í•œ ì‹¤í–‰ ê³„íšì…ë‹ˆë‹¤. (ê°œë°œ ëª¨ë“œ)",
                steps=steps,
                estimatedChanges=len(steps),
                tokensUsed=0,
            )
        
        # í”„ë¡œë•ì…˜ ëª¨ë“œ: ì‹¤ì œ LLM í˜¸ì¶œ
        try:
            llm_client = get_llm_client()
            
            # Plan í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            files_context = ""
            if file_contents:
                files_context = "\n\nê´€ë ¨ íŒŒì¼:\n" + "\n".join([
                    f"### {fp}\n```\n{content[:500]}...\n```" 
                    for fp, content in file_contents.items()
                ])
            
            messages = [
                {"role": "system", "content": """You are a planning assistant. Given a goal, create a detailed step-by-step plan.

Output format (JSON):
{
  "summary": "Brief summary of the plan",
  "steps": [
    {"stepNumber": 1, "description": "Step description", "filePath": "optional/file/path.py"},
    ...
  ]
}

Keep the plan focused and actionable. Each step should be clear and specific."""},
                {"role": "user", "content": f"Goal: {request.goal}\n\nContext: {request.context or 'None'}{files_context}"}
            ]
            
            llm_response = await llm_client.chat(messages=messages)
            response_text = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                import json
                # JSON ë¸”ë¡ ì¶”ì¶œ
                if "```json" in response_text:
                    json_str = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    json_str = response_text.split("```")[1].split("```")[0]
                else:
                    json_str = response_text
                    
                plan_data = json.loads(json_str.strip())
                steps = [
                    TaskStep(
                        stepNumber=s.get("stepNumber", i+1),
                        description=s.get("description", ""),
                        filePath=s.get("filePath"),
                    )
                    for i, s in enumerate(plan_data.get("steps", []))
                ]
                summary = plan_data.get("summary", f"Plan for: {request.goal}")
            except:
                # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì‘ë‹µ
                steps = [TaskStep(stepNumber=1, description=response_text[:500])]
                summary = f"Plan for: {request.goal}"
            
            return AIPlanResponse(
                summary=summary,
                steps=steps,
                estimatedChanges=len(steps),
                tokensUsed=tokens_used,
            )
            
        except (LLMTimeoutError, LLMError) as e:
            # LLM ì˜¤ë¥˜ì‹œ ê¸°ë³¸ ê³„íš ë°˜í™˜
            return AIPlanResponse(
                summary=f"âš ï¸ LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}",
                steps=[TaskStep(stepNumber=1, description="LLM ì„œë²„ ì—°ê²° í™•ì¸ í•„ìš”")],
                estimatedChanges=0,
                tokensUsed=0,
            )
            
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR", "detail": str(e)},
        )


def _generate_mock_plan_steps(goal: str, file_contents: dict) -> list:
    """ê°œë°œ ëª¨ë“œìš© Mock ê³„íš ë‹¨ê³„ ìƒì„±"""
    goal_lower = goal.lower()
    
    if "ë¡œê·¸ì¸" in goal_lower or "auth" in goal_lower:
        return [
            TaskStep(stepNumber=1, description="ì‚¬ìš©ì ëª¨ë¸ ë° DB ìŠ¤í‚¤ë§ˆ ì •ì˜", filePath="src/models/user.py"),
            TaskStep(stepNumber=2, description="ë¹„ë°€ë²ˆí˜¸ í•´ì‹± ìœ í‹¸ë¦¬í‹° êµ¬í˜„", filePath="src/utils/auth.py"),
            TaskStep(stepNumber=3, description="ë¡œê·¸ì¸ API ì—”ë“œí¬ì¸íŠ¸ ìƒì„±", filePath="src/routers/auth.py"),
            TaskStep(stepNumber=4, description="JWT í† í° ë°œê¸‰ ë¡œì§ êµ¬í˜„", filePath="src/services/jwt.py"),
            TaskStep(stepNumber=5, description="ë¡œê·¸ì¸ í…ŒìŠ¤íŠ¸ ì‘ì„±", filePath="tests/test_auth.py"),
        ]
    elif "í…ŒìŠ¤íŠ¸" in goal_lower or "test" in goal_lower:
        return [
            TaskStep(stepNumber=1, description="í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • (pytest)", filePath="pytest.ini"),
            TaskStep(stepNumber=2, description="ìœ ë‹› í…ŒìŠ¤íŠ¸ ì‘ì„±", filePath="tests/test_unit.py"),
            TaskStep(stepNumber=3, description="í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±", filePath="tests/test_integration.py"),
            TaskStep(stepNumber=4, description="í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ í™•ì¸"),
        ]
    elif "ë¦¬íŒ©í† ë§" in goal_lower or "refactor" in goal_lower:
        return [
            TaskStep(stepNumber=1, description="ì¤‘ë³µ ì½”ë“œ ì‹ë³„ ë° ë¶„ì„"),
            TaskStep(stepNumber=2, description="ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì¶”ì¶œ", filePath="src/utils/common.py"),
            TaskStep(stepNumber=3, description="ê¸°ì¡´ ì½”ë“œì—ì„œ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •"),
            TaskStep(stepNumber=4, description="í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° í™•ì¸"),
        ]
    else:
        # ì¼ë°˜ì ì¸ ê³„íš
        return [
            TaskStep(stepNumber=1, description=f"'{goal}' ìš”êµ¬ì‚¬í•­ ë¶„ì„"),
            TaskStep(stepNumber=2, description="í•„ìš”í•œ íŒŒì¼/ëª¨ë“ˆ ì‹ë³„"),
            TaskStep(stepNumber=3, description="ì½”ë“œ êµ¬í˜„"),
            TaskStep(stepNumber=4, description="í…ŒìŠ¤íŠ¸ ì‘ì„± ë° ì‹¤í–‰"),
            TaskStep(stepNumber=5, description="ë¬¸ì„œí™”"),
        ]


# ============================================================
# AI Agent Mode - ìë™ ì½”ë“œ ì‘ì„±/ìˆ˜ì •
# ============================================================

@router.post(
    "/agent",
    response_model=AIAgentResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ìë™ ì½”ë“œ ì‘ì„±/ìˆ˜ì • (Agent ëª¨ë“œ)",
    description="ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ë³€ê²½ ì‚¬í•­ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
)
async def run_agent(request: AIAgentRequest):
    """
    ì—ì´ì „íŠ¸ ëª¨ë“œ: ì‚¬ìš©ì ì§€ì‹œì— ë”°ë¼ ìë™ìœ¼ë¡œ ì½”ë“œë¥¼ ë¶„ì„í•˜ê³ ,
    í•„ìš”í•œ íŒŒì¼ì„ ìˆ˜ì •/ìƒì„±/ì‚­ì œí•˜ëŠ” ë³€ê²½ ì‚¬í•­ì„ ì œì•ˆí•©ë‹ˆë‹¤.
    
    auto_apply=Trueì¸ ê²½ìš° ë³€ê²½ ì‚¬í•­ì„ ì§ì ‘ ì ìš©í•©ë‹ˆë‹¤.
    (âš ï¸ ì£¼ì˜: í˜„ì¬ PoCì—ì„œëŠ” auto_applyëŠ” ë¬´ì‹œë©ë‹ˆë‹¤)
    """
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    try:
        dev_mode = settings.DEV_MODE
        
        # ê´€ë ¨ íŒŒì¼ ë‚´ìš© ìˆ˜ì§‘
        file_contents = {}
        if request.file_paths:
            for fp in request.file_paths[:10]:  # ìµœëŒ€ 10ê°œ íŒŒì¼
                if _validate_path(fp):
                    try:
                        content = await _read_file_content(request.workspace_id, fp)
                        file_contents[fp] = content
                    except:
                        pass
        
        if dev_mode:
            # ê°œë°œ ëª¨ë“œ: Mock ì‘ë‹µ ìƒì„±
            changes = _generate_mock_agent_changes(request.instruction, file_contents)
            return AIAgentResponse(
                summary=f"'{request.instruction}' ì‘ì—… ì™„ë£Œ (ê°œë°œ ëª¨ë“œ)",
                changes=changes,
                applied=False,
                tokensUsed=0,
            )
        
        # í”„ë¡œë•ì…˜ ëª¨ë“œ: ì‹¤ì œ LLM í˜¸ì¶œ
        try:
            llm_client = get_llm_client()
            
            # Agent í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            files_context = ""
            if file_contents:
                files_context = "\n\nFiles to modify:\n" + "\n".join([
                    f"### {fp}\n```\n{content}\n```" 
                    for fp, content in file_contents.items()
                ])
            
            messages = [
                {"role": "system", "content": """You are a coding agent. Given an instruction, analyze the code and provide specific changes.

Output format (JSON):
{
  "summary": "What was done",
  "changes": [
    {
      "filePath": "path/to/file.py",
      "action": "modify|create|delete",
      "diff": "--- a/file.py\\n+++ b/file.py\\n@@ -1,3 +1,4 @@\\n...",
      "description": "What this change does"
    }
  ]
}

Be specific and provide actual code changes in unified diff format."""},
                {"role": "user", "content": f"Instruction: {request.instruction}{files_context}"}
            ]
            
            llm_response = await llm_client.chat(messages=messages)
            response_text = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                import json
                if "```json" in response_text:
                    json_str = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    json_str = response_text.split("```")[1].split("```")[0]
                else:
                    json_str = response_text
                    
                agent_data = json.loads(json_str.strip())
                changes = [
                    FileChange(
                        filePath=c.get("filePath", "unknown"),
                        action=c.get("action", "modify"),
                        diff=c.get("diff"),
                        description=c.get("description", ""),
                    )
                    for c in agent_data.get("changes", [])
                ]
                summary = agent_data.get("summary", "Changes generated")
            except:
                changes = [FileChange(
                    filePath="unknown",
                    action="modify",
                    description=response_text[:500],
                )]
                summary = "Agent response (parsing failed)"
            
            return AIAgentResponse(
                summary=summary,
                changes=changes,
                applied=False,  # PoCì—ì„œëŠ” ìë™ ì ìš© ë¹„í™œì„±í™”
                tokensUsed=tokens_used,
            )
            
        except (LLMTimeoutError, LLMError) as e:
            return AIAgentResponse(
                summary=f"âš ï¸ LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}",
                changes=[],
                applied=False,
                tokensUsed=0,
            )
            
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR", "detail": str(e)},
        )


def _generate_mock_agent_changes(instruction: str, file_contents: dict) -> list:
    """ê°œë°œ ëª¨ë“œìš© Mock Agent ë³€ê²½ ì‚¬í•­ ìƒì„±"""
    changes = []
    
    if "ì£¼ì„" in instruction or "comment" in instruction.lower():
        for fp, content in file_contents.items():
            lines = content.split("\n")
            diff_lines = [f"--- a/{fp}", f"+++ b/{fp}", "@@ -1,3 +1,4 @@"]
            if lines:
                diff_lines.append(f"+# {instruction}")
                diff_lines.append(f" {lines[0]}")
            
            changes.append(FileChange(
                filePath=fp,
                action="modify",
                diff="\n".join(diff_lines),
                description=f"{fp}ì— ì£¼ì„ ì¶”ê°€",
            ))
    elif "ìƒì„±" in instruction or "create" in instruction.lower():
        changes.append(FileChange(
            filePath="new_file.py",
            action="create",
            diff="""--- /dev/null
+++ b/new_file.py
@@ -0,0 +1,5 @@
+\"\"\"
+Auto-generated file
+\"\"\"
+
+# TODO: Implement
""",
            description="ìƒˆ íŒŒì¼ ìƒì„±",
        ))
    else:
        # ê¸°ì¡´ íŒŒì¼ ìˆ˜ì • ì œì•ˆ
        for fp in list(file_contents.keys())[:2]:
            changes.append(FileChange(
                filePath=fp,
                action="modify",
                description=f"'{instruction}'ì— ë”°ë¼ {fp} ìˆ˜ì • í•„ìš” (ê°œë°œ ëª¨ë“œ)",
            ))
    
    if not changes:
        changes.append(FileChange(
            filePath="example.py",
            action="modify",
            description=f"'{instruction}' ì‘ì—…ì„ ìœ„í•œ ë³€ê²½ ì‚¬í•­ (ê°œë°œ ëª¨ë“œ)",
        ))
    
    return changes


# ============================================================
# AI Debug Mode - ë²„ê·¸ ë¶„ì„/ìˆ˜ì •
# ============================================================

@router.post(
    "/debug",
    response_model=AIDebugResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ë²„ê·¸ ë¶„ì„/ìˆ˜ì • (Debug ëª¨ë“œ)",
    description="ì—ëŸ¬ ë©”ì‹œì§€, ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤, ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ ë²„ê·¸ ì›ì¸ê³¼ í•´ê²°ì±…ì„ ì œì‹œí•©ë‹ˆë‹¤.",
)
async def debug_code(request: AIDebugRequest):
    """
    ë””ë²„ê·¸ ëª¨ë“œ: ì—ëŸ¬ ë©”ì‹œì§€, ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤, ê´€ë ¨ ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬
    ë²„ê·¸ì˜ ì›ì¸ì„ ì§„ë‹¨í•˜ê³  ìˆ˜ì • ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.
    """
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    if not request.error_message and not request.stack_trace and not request.description:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "At least one of error_message, stack_trace, or description is required", "code": "DEBUG_NO_INPUT"},
        )
    
    try:
        dev_mode = settings.DEV_MODE
        
        # íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        file_content = None
        if request.file_path and _validate_path(request.file_path):
            try:
                if request.file_content:
                    file_content = request.file_content
                else:
                    file_content = await _read_file_content(request.workspace_id, request.file_path)
            except:
                pass
        
        if dev_mode:
            # ê°œë°œ ëª¨ë“œ: Mock ì§„ë‹¨ ìƒì„±
            diagnosis, root_cause, fixes, tips = _generate_mock_debug_response(
                error_message=request.error_message,
                stack_trace=request.stack_trace,
                file_path=request.file_path,
                file_content=file_content,
                description=request.description,
            )
            return AIDebugResponse(
                diagnosis=diagnosis,
                rootCause=root_cause,
                fixes=fixes,
                preventionTips=tips,
                tokensUsed=0,
            )
        
        # í”„ë¡œë•ì…˜ ëª¨ë“œ: ì‹¤ì œ LLM í˜¸ì¶œ
        try:
            llm_client = get_llm_client()
            
            # Debug í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            debug_context = []
            if request.error_message:
                debug_context.append(f"Error Message:\n{request.error_message}")
            if request.stack_trace:
                debug_context.append(f"Stack Trace:\n{request.stack_trace}")
            if request.description:
                debug_context.append(f"Description:\n{request.description}")
            if file_content:
                debug_context.append(f"Code ({request.file_path}):\n```\n{file_content}\n```")
            
            messages = [
                {"role": "system", "content": """You are a debugging expert. Analyze the error and provide a diagnosis.

Output format (JSON):
{
  "diagnosis": "Detailed analysis of the problem",
  "rootCause": "The root cause of the bug",
  "fixes": [
    {
      "filePath": "path/to/file.py",
      "lineNumber": 42,
      "originalCode": "old code",
      "fixedCode": "new code",
      "explanation": "Why this fixes the issue"
    }
  ],
  "preventionTips": ["Tip 1", "Tip 2"]
}

Be thorough and specific in your analysis."""},
                {"role": "user", "content": "\n\n".join(debug_context)}
            ]
            
            llm_response = await llm_client.chat(messages=messages)
            response_text = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                import json
                if "```json" in response_text:
                    json_str = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    json_str = response_text.split("```")[1].split("```")[0]
                else:
                    json_str = response_text
                    
                debug_data = json.loads(json_str.strip())
                fixes = [
                    BugFix(
                        filePath=f.get("filePath", request.file_path or "unknown"),
                        lineNumber=f.get("lineNumber"),
                        originalCode=f.get("originalCode", ""),
                        fixedCode=f.get("fixedCode", ""),
                        explanation=f.get("explanation", ""),
                    )
                    for f in debug_data.get("fixes", [])
                ]
                diagnosis = debug_data.get("diagnosis", "Analysis complete")
                root_cause = debug_data.get("rootCause", "Unknown")
                tips = debug_data.get("preventionTips", [])
            except:
                diagnosis = response_text[:500]
                root_cause = "LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"
                fixes = []
                tips = []
            
            return AIDebugResponse(
                diagnosis=diagnosis,
                rootCause=root_cause,
                fixes=fixes,
                preventionTips=tips,
                tokensUsed=tokens_used,
            )
            
        except (LLMTimeoutError, LLMError) as e:
            return AIDebugResponse(
                diagnosis=f"âš ï¸ LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}",
                rootCause="LLM ì„œë¹„ìŠ¤ ì—°ê²° í•„ìš”",
                fixes=[],
                preventionTips=["vLLM ì„œë²„ ìƒíƒœ í™•ì¸", "VLLM_BASE_URL í™˜ê²½ë³€ìˆ˜ í™•ì¸"],
                tokensUsed=0,
            )
            
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR", "detail": str(e)},
        )


def _generate_mock_debug_response(
    error_message: str | None,
    stack_trace: str | None,
    file_path: str | None,
    file_content: str | None,
    description: str | None,
) -> tuple:
    """ê°œë°œ ëª¨ë“œìš© Mock ë””ë²„ê·¸ ì‘ë‹µ ìƒì„±"""
    
    # ì—ëŸ¬ ìœ í˜• ë¶„ì„
    error_lower = (error_message or "").lower() + (stack_trace or "").lower()
    
    if "typeerror" in error_lower or "type" in error_lower:
        diagnosis = "TypeErrorê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë³€ìˆ˜ì˜ íƒ€ì…ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤."
        root_cause = "í•¨ìˆ˜ì— ì˜ëª»ëœ íƒ€ì…ì˜ ì¸ìê°€ ì „ë‹¬ë˜ì—ˆê±°ë‚˜, None ê°’ì— ëŒ€í•´ ë©”ì„œë“œë¥¼ í˜¸ì¶œí–ˆìŠµë‹ˆë‹¤."
        fixes = [BugFix(
            filePath=file_path or "unknown.py",
            lineNumber=1,
            originalCode="result = data.process()",
            fixedCode="result = data.process() if data is not None else None",
            explanation="None ì²´í¬ë¥¼ ì¶”ê°€í•˜ì—¬ TypeError ë°©ì§€",
        )]
        tips = ["íƒ€ì… íŒíŠ¸ ì‚¬ìš©", "None ì²´í¬ ì¶”ê°€", "isinstance()ë¡œ íƒ€ì… ê²€ì¦"]
        
    elif "importerror" in error_lower or "modulenotfound" in error_lower:
        diagnosis = "ëª¨ë“ˆ ì„í¬íŠ¸ ì˜¤ë¥˜ì…ë‹ˆë‹¤. í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤."
        root_cause = "íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜, ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        fixes = [BugFix(
            filePath="requirements.txt",
            lineNumber=None,
            originalCode="",
            fixedCode="missing_package>=1.0.0",
            explanation="í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ requirements.txtì— ì¶”ê°€",
        )]
        tips = ["pip install ì‹¤í–‰", "ê°€ìƒí™˜ê²½ í™•ì¸", "PYTHONPATH í™•ì¸"]
        
    elif "syntaxerror" in error_lower:
        diagnosis = "ë¬¸ë²• ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì½”ë“œì— êµ¬ë¬¸ ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤."
        root_cause = "ê´„í˜¸, ì½œë¡ , ë“¤ì—¬ì“°ê¸° ë“± Python ë¬¸ë²• ê·œì¹™ ìœ„ë°˜"
        fixes = [BugFix(
            filePath=file_path or "unknown.py",
            lineNumber=1,
            originalCode="def func(",
            fixedCode="def func():",
            explanation="í•¨ìˆ˜ ì •ì˜ ë¬¸ë²• ìˆ˜ì •",
        )]
        tips = ["IDEì˜ ë¦°í„° í™œì„±í™”", "ì½”ë“œ í¬ë§·í„° ì‚¬ìš©", "ê´„í˜¸ ì§ í™•ì¸"]
        
    else:
        diagnosis = f"""## ë””ë²„ê·¸ ë¶„ì„ (ê°œë°œ ëª¨ë“œ)

**ì—ëŸ¬**: {error_message or 'ì—†ìŒ'}

**ì„¤ëª…**: {description or 'ì—†ìŒ'}

{'**ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤**:' + chr(10) + '```' + chr(10) + stack_trace[:500] + chr(10) + '```' if stack_trace else ''}

ì´ ë¶„ì„ì€ ê°œë°œ ëª¨ë“œì—ì„œ ìƒì„±ëœ Mock ì‘ë‹µì…ë‹ˆë‹¤.
ì‹¤ì œ AI ë¶„ì„ì„ ìœ„í•´ vLLM ì„œë²„ë¥¼ ì—°ê²°í•´ì£¼ì„¸ìš”."""
        root_cause = "ê°œë°œ ëª¨ë“œì—ì„œëŠ” ì •í™•í•œ ì›ì¸ ë¶„ì„ì´ ì œí•œë©ë‹ˆë‹¤."
        fixes = []
        tips = ["vLLM ì„œë²„ ì—°ê²° í›„ ì¬ì‹œë„", "ì—ëŸ¬ ë¡œê·¸ ìì„¸íˆ í™•ì¸", "ê´€ë ¨ ì½”ë“œ ê²€í† "]
    
    return diagnosis, root_cause, fixes, tips


# ============================================================
# AI Mode Status
# ============================================================

@router.get(
    "/modes",
    summary="ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë“œ ëª©ë¡",
    description="í˜„ì¬ ì§€ì›í•˜ëŠ” AI ëª¨ë“œ ëª©ë¡ê³¼ ì„¤ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
)
async def get_available_modes():
    """ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë“œ ëª©ë¡ ë°˜í™˜"""
    return {
        "modes": [
            {
                "id": "ask",
                "name": "Ask",
                "description": "ì½”ë“œì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  ë‹µë³€ì„ ë°›ìŠµë‹ˆë‹¤.",
                "icon": "ğŸ’¬",
            },
            {
                "id": "agent",
                "name": "Agent",
                "description": "ìë™ìœ¼ë¡œ ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ë³€ê²½ ì‚¬í•­ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
                "icon": "ğŸ¤–",
            },
            {
                "id": "plan",
                "name": "Plan",
                "description": "ëª©í‘œë¥¼ ë¶„ì„í•˜ê³  ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤.",
                "icon": "ğŸ“‹",
            },
            {
                "id": "debug",
                "name": "Debug",
                "description": "ì—ëŸ¬ë¥¼ ë¶„ì„í•˜ê³  ë²„ê·¸ ìˆ˜ì • ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.",
                "icon": "ğŸ›",
            },
        ],
        "current": "ask",  # ê¸°ë³¸ ëª¨ë“œ
    }


# ============================================================
# Advanced AI Chat with Context & Image (Cursor-like)
# ============================================================

from fastapi import UploadFile, File, Form
from ..models import (
    ContextType,
    ContextItem,
    AIAdvancedChatRequest,
    AIAdvancedChatResponse,
    ImageUploadResponse,
    ImageAnalysisRequest,
    ImageAnalysisResponse,
    ContextSuggestRequest,
    ContextSuggestion,
    ContextSuggestResponse,
)
import uuid
import base64
from pathlib import Path
import os

# ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’)
IMAGE_UPLOAD_DIR = Path(os.getenv("IMAGE_UPLOAD_DIR", "/tmp/ai_images"))
IMAGE_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# í—ˆìš©ëœ ì´ë¯¸ì§€ í˜•ì‹
ALLOWED_IMAGE_TYPES = {"image/jpeg", "image/png", "image/gif", "image/webp"}
MAX_IMAGE_SIZE = 10 * 1024 * 1024  # 10MB


@router.post(
    "/image/upload",
    response_model=ImageUploadResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid image"},
        413: {"model": ErrorResponse, "description": "Image too large"},
    },
    summary="ì´ë¯¸ì§€ ì—…ë¡œë“œ",
    description="AI ë¶„ì„ì„ ìœ„í•œ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤.",
)
async def upload_image(
    file: UploadFile = File(..., description="ì—…ë¡œë“œí•  ì´ë¯¸ì§€ íŒŒì¼"),
):
    """
    ì´ë¯¸ì§€ ì—…ë¡œë“œ
    
    - ìŠ¤í¬ë¦°ìƒ·, ì—ëŸ¬ í™”ë©´, UI ë””ìì¸ ë“±ì„ ì—…ë¡œë“œ
    - AIê°€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì½”ë“œ ì‘ì„±ì— í™œìš©
    """
    # MIME íƒ€ì… ê²€ì¦
    if file.content_type not in ALLOWED_IMAGE_TYPES:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": f"Invalid image type. Allowed: {', '.join(ALLOWED_IMAGE_TYPES)}", "code": "INVALID_IMAGE_TYPE"},
        )
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    content = await file.read()
    if len(content) > MAX_IMAGE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail={"error": f"Image too large. Max size: {MAX_IMAGE_SIZE // 1024 // 1024}MB", "code": "IMAGE_TOO_LARGE"},
        )
    
    # ê³ ìœ  ID ìƒì„±
    image_id = str(uuid.uuid4())
    ext = file.content_type.split("/")[-1]
    if ext == "jpeg":
        ext = "jpg"
    
    # íŒŒì¼ ì €ì¥
    file_path = IMAGE_UPLOAD_DIR / f"{image_id}.{ext}"
    with open(file_path, "wb") as f:
        f.write(content)
    
    # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´ (PIL ì—†ì´ ê¸°ë³¸ê°’ ì‚¬ìš©)
    width, height = None, None
    try:
        # PILì´ ìˆìœ¼ë©´ ì‚¬ìš©
        from PIL import Image
        with Image.open(file_path) as img:
            width, height = img.size
    except ImportError:
        pass
    
    # ìƒëŒ€ URL ìƒì„±
    image_url = f"/api/ai/image/{image_id}.{ext}"
    
    return ImageUploadResponse(
        image_id=image_id,
        image_url=image_url,
        thumbnail_url=image_url,  # ì¸ë„¤ì¼ ìƒì„± ë¯¸êµ¬í˜„
        mime_type=file.content_type,
        size=len(content),
        width=width,
        height=height,
    )


@router.get(
    "/image/{image_filename}",
    summary="ì´ë¯¸ì§€ ì¡°íšŒ",
    description="ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
)
async def get_image(image_filename: str):
    """ì—…ë¡œë“œëœ ì´ë¯¸ì§€ ì¡°íšŒ"""
    from fastapi.responses import FileResponse
    
    # ê²½ë¡œ íƒˆì¶œ ë°©ì§€
    if ".." in image_filename or "/" in image_filename:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Invalid filename", "code": "INVALID_PATH"},
        )
    
    file_path = IMAGE_UPLOAD_DIR / image_filename
    if not file_path.exists():
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail={"error": "Image not found", "code": "IMAGE_NOT_FOUND"},
        )
    
    return FileResponse(file_path)


@router.post(
    "/image/analyze",
    response_model=ImageAnalysisResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        404: {"model": ErrorResponse, "description": "Image not found"},
    },
    summary="ì´ë¯¸ì§€ ë¶„ì„",
    description="ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ AIë¡œ ë¶„ì„í•©ë‹ˆë‹¤.",
)
async def analyze_image(request: ImageAnalysisRequest):
    """
    ì´ë¯¸ì§€ ë¶„ì„
    
    - ìŠ¤í¬ë¦°ìƒ·ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ì¶œ
    - UI ë””ìì¸ì—ì„œ ì½”ë“œ ìƒì„±
    - ë‹¤ì´ì–´ê·¸ë¨ í•´ì„
    
    Vision LLM ì§€ì›:
    - LiteLLMì„ í†µí•œ GPT-4V, Claude Vision, LLaVA ë“±
    - VISION_MODEL í™˜ê²½ë³€ìˆ˜ë¡œ ëª¨ë¸ ì§€ì •
    """
    import base64
    import httpx
    
    # ì¤‘ì•™ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    dev_mode = settings.DEV_MODE
    litellm_url = settings.LITELLM_BASE_URL
    vision_model = settings.VISION_MODEL
    
    # ê°œë°œ ëª¨ë“œ Mock ì‘ë‹µ
    if dev_mode:
        question = request.question or "ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
        return ImageAnalysisResponse(
            description=f"""## ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ (ê°œë°œ ëª¨ë“œ)

**ì§ˆë¬¸**: {question}

ì´ ì‘ë‹µì€ ê°œë°œ ëª¨ë“œì—ì„œ ìƒì„±ëœ Mock ì‘ë‹µì…ë‹ˆë‹¤.

ì‹¤ì œ ì´ë¯¸ì§€ ë¶„ì„ì„ ìœ„í•´ì„œëŠ”:
1. VISION_MODEL í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ì˜ˆ: gpt-4-vision-preview, claude-3-opus)
2. DEV_MODE=false ì„¤ì •
3. LiteLLM í”„ë¡ì‹œ ì—°ê²° í™•ì¸

**ì§€ì› ê¸°ëŠ¥**:
- ìŠ¤í¬ë¦°ìƒ·ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ì¶œ (OCR)
- UI ë””ìì¸ì—ì„œ ì½”ë“œ ìƒì„±
- ë‹¤ì´ì–´ê·¸ë¨/í”Œë¡œìš°ì°¨íŠ¸ í•´ì„
- ì½”ë“œ ìŠ¤ë‹ˆí« ì¶”ì¶œ
""",
            extracted_text="[ê°œë°œ ëª¨ë“œ] OCR í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
            code_blocks=["# ê°œë°œ ëª¨ë“œì—ì„œëŠ” ì½”ë“œ ì¶”ì¶œì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."],
        )
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì½ê¸° ë° Base64 ì¸ì½”ë”©
    image_id = request.image_id
    upload_dir = Path(os.getenv("UPLOAD_DIR", "/tmp/uploads"))
    image_path = upload_dir / image_id
    
    if not image_path.exists():
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail={"error": "Image not found", "code": "IMAGE_NOT_FOUND"},
        )
    
    # ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©
    try:
        with open(image_path, "rb") as f:
            image_data = base64.b64encode(f.read()).decode("utf-8")
        
        # íŒŒì¼ í™•ì¥ìë¡œ MIME íƒ€ì… ì¶”ì •
        suffix = image_path.suffix.lower()
        mime_type = {
            ".jpg": "image/jpeg",
            ".jpeg": "image/jpeg",
            ".png": "image/png",
            ".gif": "image/gif",
            ".webp": "image/webp",
        }.get(suffix, "image/png")
        
        image_url = f"data:{mime_type};base64,{image_data}"
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": f"Failed to read image: {e}", "code": "IMAGE_READ_ERROR"},
        )
    
    # Vision LLM ìš”ì²­ (OpenAI í˜¸í™˜ í˜•ì‹)
    question = request.question or "ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. ì—ëŸ¬ ë©”ì‹œì§€ê°€ ìˆë‹¤ë©´ ì¶”ì¶œí•˜ê³ , ì½”ë“œê°€ ìˆë‹¤ë©´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": question},
                {"type": "image_url", "image_url": {"url": image_url}},
            ],
        }
    ]
    
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{litellm_url}/v1/chat/completions",
                json={
                    "model": vision_model,
                    "messages": messages,
                    "max_tokens": 2000,
                },
                headers={"Content-Type": "application/json"},
            )
            
            if response.status_code != 200:
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail={"error": f"Vision LLM error: {response.text}", "code": "VISION_LLM_ERROR"},
                )
            
            result = response.json()
            description = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            
            # ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ (```ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¶€ë¶„)
            import re
            code_pattern = r"```(?:\w+)?\n(.*?)```"
            code_blocks = re.findall(code_pattern, description, re.DOTALL)
            
            return ImageAnalysisResponse(
                description=description,
                extracted_text=None,  # OCR ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì—
                code_blocks=code_blocks if code_blocks else None,
            )
            
    except httpx.TimeoutException:
        raise HTTPException(
            status_code=status.HTTP_504_GATEWAY_TIMEOUT,
            detail={"error": "Vision LLM timeout", "code": "VISION_LLM_TIMEOUT"},
        )
    except httpx.RequestError as e:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail={"error": f"Vision LLM connection error: {e}", "code": "VISION_LLM_CONNECTION_ERROR"},
        )


@router.post(
    "/context/suggest",
    response_model=ContextSuggestResponse,
    summary="ì»¨í…ìŠ¤íŠ¸ ì œì•ˆ",
    description="ì…ë ¥ì— ë”°ë¼ ê´€ë ¨ íŒŒì¼/ì½”ë“œë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.",
)
async def suggest_context(request: ContextSuggestRequest):
    """
    ì»¨í…ìŠ¤íŠ¸ ì œì•ˆ (@ ì…ë ¥ ì‹œ)
    
    ì‚¬ìš©ìê°€ "@"ë¥¼ ì…ë ¥í•˜ë©´ ê´€ë ¨ íŒŒì¼, í´ë”, ì‹¬ë³¼ì„ ì œì•ˆí•©ë‹ˆë‹¤.
    """
    import os
    from pathlib import Path
    
    workspace_root = get_workspace_root(request.workspace_id)
    query = request.query.lower()
    suggestions = []
    
    # íŒŒì¼ ê²€ìƒ‰
    if not workspace_exists(workspace_root):
        return ContextSuggestResponse(suggestions=[], total=0)
    
    try:
        for root, dirs, files in os.walk(workspace_root):
            # ìˆ¨ê¹€ í´ë” ì œì™¸
            dirs[:] = [d for d in dirs if not d.startswith(".")]
            
            rel_root = Path(root).relative_to(workspace_root)
            
            # í´ë” ê²€ìƒ‰
            if not request.types or ContextType.FOLDER in request.types:
                for d in dirs:
                    if query in d.lower():
                        path = str(rel_root / d)
                        suggestions.append(ContextSuggestion(
                            type=ContextType.FOLDER,
                            path=path,
                            name=d,
                            preview=f"ğŸ“ {path}",
                            relevance=0.8 if d.lower().startswith(query) else 0.5,
                        ))
            
            # íŒŒì¼ ê²€ìƒ‰
            if not request.types or ContextType.FILE in request.types:
                for f in files:
                    if query in f.lower():
                        path = str(rel_root / f)
                        suggestions.append(ContextSuggestion(
                            type=ContextType.FILE,
                            path=path,
                            name=f,
                            preview=f"ğŸ“„ {path}",
                            relevance=0.9 if f.lower().startswith(query) else 0.6,
                        ))
            
            # ë„ˆë¬´ ë§ì´ íƒìƒ‰í•˜ì§€ ì•Šë„ë¡ ì œí•œ
            if len(suggestions) >= request.limit * 2:
                break
    except Exception:
        pass
    
    # ê´€ë ¨ë„ìˆœ ì •ë ¬ ë° ì œí•œ
    suggestions.sort(key=lambda x: x.relevance, reverse=True)
    suggestions = suggestions[:request.limit]
    
    return ContextSuggestResponse(suggestions=suggestions, total=len(suggestions))


@router.post(
    "/advanced/chat",
    response_model=AIAdvancedChatResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ê³ ê¸‰ AI ì±„íŒ… (Cursor ìŠ¤íƒ€ì¼)",
    description="ì—¬ëŸ¬ ì»¨í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ë¥¼ í¬í•¨í•œ AI ì±„íŒ…ì…ë‹ˆë‹¤.",
)
async def advanced_chat(request: AIAdvancedChatRequest):
    """
    ê³ ê¸‰ AI ì±„íŒ…
    
    - ì—¬ëŸ¬ íŒŒì¼/í´ë”ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€
    - ì´ë¯¸ì§€ ì²¨ë¶€
    - í´ë¦½ë³´ë“œ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€
    - Agent/Plan/Debug/Ask ëª¨ë“œ ì§€ì›
    """
    import os
    dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
    
    # ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
    context_parts = []
    
    # í˜„ì¬ ì—´ë¦° íŒŒì¼
    if request.current_file and request.current_content:
        context_parts.append(f"**í˜„ì¬ íŒŒì¼ ({request.current_file})**:\n```\n{request.current_content[:2000]}\n```")
        if request.current_selection:
            lines = request.current_content.split("\n")
            selected = "\n".join(lines[request.current_selection.start_line - 1:request.current_selection.end_line])
            context_parts.append(f"**ì„ íƒëœ ì½”ë“œ (L{request.current_selection.start_line}-{request.current_selection.end_line})**:\n```\n{selected}\n```")
    
    # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
    if request.contexts:
        for ctx in request.contexts:
            if ctx.type == ContextType.FILE and ctx.content:
                context_parts.append(f"**íŒŒì¼ ({ctx.path or ctx.name})**:\n```\n{ctx.content[:1500]}\n```")
            elif ctx.type == ContextType.IMAGE:
                if ctx.image_url:
                    context_parts.append(f"**ì´ë¯¸ì§€**: {ctx.image_url}")
                elif ctx.image_base64:
                    context_parts.append(f"**ì´ë¯¸ì§€ (Base64)**: [ì´ë¯¸ì§€ ì²¨ë¶€ë¨]")
            elif ctx.type == ContextType.CLIPBOARD and ctx.content:
                context_parts.append(f"**í´ë¦½ë³´ë“œ**:\n```\n{ctx.content[:1000]}\n```")
            elif ctx.type == ContextType.SELECTION and ctx.content:
                context_parts.append(f"**ì„ íƒ ì˜ì—­**:\n```\n{ctx.content[:1000]}\n```")
    
    full_context = "\n\n".join(context_parts) if context_parts else ""
    
    # ëª¨ë“œë³„ ì²˜ë¦¬
    if dev_mode:
        if request.mode == AIMode.PLAN:
            return AIAdvancedChatResponse(
                response=f"""## ì‘ì—… ê³„íš (ê°œë°œ ëª¨ë“œ)

**ëª©í‘œ**: {request.message}

{f'**ì»¨í…ìŠ¤íŠ¸**:{chr(10)}{full_context[:500]}...' if full_context else ''}

### ì‹¤í–‰ ê³„íš:
1. ìš”êµ¬ì‚¬í•­ ë¶„ì„
2. ì„¤ê³„ ê²€í† 
3. êµ¬í˜„
4. í…ŒìŠ¤íŠ¸
5. ë°°í¬

*ì´ ì‘ë‹µì€ ê°œë°œ ëª¨ë“œì—ì„œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤ì œ AI ë¶„ì„ì„ ìœ„í•´ vLLMì„ ì—°ê²°í•˜ì„¸ìš”.*
""",
                mode=AIMode.PLAN,
                tokens_used=0,
                plan_steps=[
                    TaskStep(step_number=1, description="ìš”êµ¬ì‚¬í•­ ë¶„ì„", status="pending"),
                    TaskStep(step_number=2, description="ì„¤ê³„ ê²€í† ", status="pending"),
                    TaskStep(step_number=3, description="êµ¬í˜„", status="pending"),
                    TaskStep(step_number=4, description="í…ŒìŠ¤íŠ¸", status="pending"),
                    TaskStep(step_number=5, description="ë°°í¬", status="pending"),
                ],
            )
        elif request.mode == AIMode.AGENT:
            return AIAdvancedChatResponse(
                response=f"""## ì½”ë“œ ë³€ê²½ ì œì•ˆ (ê°œë°œ ëª¨ë“œ)

**ìš”ì²­**: {request.message}

{f'**ì°¸ì¡°í•œ ì»¨í…ìŠ¤íŠ¸**: {len(request.contexts or [])}ê°œ' if request.contexts else ''}

### ë³€ê²½ ì‚¬í•­:
ì•„ë˜ëŠ” ê°œë°œ ëª¨ë“œ Mock ì‘ë‹µì…ë‹ˆë‹¤.

*ì‹¤ì œ ì½”ë“œ ìƒì„±ì„ ìœ„í•´ vLLMì„ ì—°ê²°í•˜ì„¸ìš”.*
""",
                mode=AIMode.AGENT,
                tokens_used=0,
                file_changes=[
                    FileChange(
                        file_path=request.current_file or "example.py",
                        action="modify",
                        content="# Agent ëª¨ë“œ (ê°œë°œ)\n# ì‹¤ì œ ë³€ê²½ ì‚¬í•­ì€ vLLM ì—°ê²° í›„ ìƒì„±ë©ë‹ˆë‹¤.",
                        diff="@@ -1 +1,2 @@\n+# Agent ëª¨ë“œ ì˜ˆì‹œ",
                    )
                ],
            )
        elif request.mode == AIMode.DEBUG:
            return AIAdvancedChatResponse(
                response=f"""## ë””ë²„ê·¸ ë¶„ì„ (ê°œë°œ ëª¨ë“œ)

**ë¬¸ì œ**: {request.message}

### ë¶„ì„:
ê°œë°œ ëª¨ë“œì—ì„œëŠ” ì œí•œì ì¸ ë¶„ì„ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### ì œì•ˆ:
1. ì—ëŸ¬ ë¡œê·¸ í™•ì¸
2. ê´€ë ¨ ì½”ë“œ ê²€í† 
3. vLLM ì—°ê²° í›„ ìƒì„¸ ë¶„ì„

*ì‹¤ì œ ë””ë²„ê¹…ì„ ìœ„í•´ vLLMì„ ì—°ê²°í•˜ì„¸ìš”.*
""",
                mode=AIMode.DEBUG,
                tokens_used=0,
                bug_fixes=[
                    BugFix(
                        filePath=request.current_file or "unknown.py",
                        lineNumber=None,
                        originalCode="",
                        fixedCode="# ìˆ˜ì • ì½”ë“œëŠ” vLLM ì—°ê²° í›„ ìƒì„±ë©ë‹ˆë‹¤",
                        explanation="ê°œë°œ ëª¨ë“œì—ì„œëŠ” ì‹¤ì œ ë²„ê·¸ ìˆ˜ì •ì´ ìƒì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                    )
                ],
            )
        else:  # ASK ëª¨ë“œ
            return AIAdvancedChatResponse(
                response=f"""## ë‹µë³€ (ê°œë°œ ëª¨ë“œ)

**ì§ˆë¬¸**: {request.message}

{f'**ì»¨í…ìŠ¤íŠ¸**: {len(request.contexts or [])}ê°œ í•­ëª© ì°¸ì¡°' if request.contexts else ''}

### ì‘ë‹µ:
ê°œë°œ ëª¨ë“œì—ì„œ ìƒì„±ëœ Mock ì‘ë‹µì…ë‹ˆë‹¤.

{f'í˜„ì¬ íŒŒì¼: `{request.current_file}`' if request.current_file else ''}

ì‹¤ì œ AI ë‹µë³€ì„ ìœ„í•´ vLLM ì„œë²„ë¥¼ ì—°ê²°í•´ì£¼ì„¸ìš”.

*VLLM_BASE_URL í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê³  DEV_MODE=falseë¡œ ë³€ê²½í•˜ì„¸ìš”.*
""",
                mode=AIMode.ASK,
                tokens_used=0,
            )
    
    # ì‹¤ì œ LLM í˜¸ì¶œ (ë¹„ê°œë°œ ëª¨ë“œ)
    try:
        llm_client = get_llm_client()
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        mode_prompts = {
            AIMode.ASK: "You are a helpful coding assistant. Answer questions about code clearly and concisely.",
            AIMode.AGENT: "You are a coding agent. Analyze the code and suggest specific changes. Provide full code for modifications.",
            AIMode.PLAN: "You are a project planner. Break down the task into clear, actionable steps.",
            AIMode.DEBUG: "You are a debugging expert. Analyze errors and suggest specific fixes.",
        }
        
        system_prompt = mode_prompts.get(request.mode, mode_prompts[AIMode.ASK])
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        
        # íˆìŠ¤í† ë¦¬ ì¶”ê°€
        if request.history:
            for msg in request.history[-10:]:  # ìµœê·¼ 10ê°œ
                messages.append({"role": msg.role, "content": msg.content})
        
        # ì»¨í…ìŠ¤íŠ¸ + ì‚¬ìš©ì ë©”ì‹œì§€
        user_message = request.message
        if full_context:
            user_message = f"{full_context}\n\n---\n\n**ì‚¬ìš©ì ìš”ì²­**: {request.message}"
        
        messages.append({"role": "user", "content": user_message})
        
        # LLM í˜¸ì¶œ
        llm_response = await llm_client.chat(messages=messages)
        
        # OpenAI API ì‘ë‹µ í˜•ì‹ì—ì„œ content ì¶”ì¶œ
        response_content = ""
        tokens_used = 0
        
        if "choices" in llm_response and len(llm_response["choices"]) > 0:
            response_content = llm_response["choices"][0].get("message", {}).get("content", "")
        
        if "usage" in llm_response:
            tokens_used = llm_response["usage"].get("total_tokens", 0)
        
        return AIAdvancedChatResponse(
            response=response_content,
            mode=request.mode,
            tokens_used=tokens_used,
        )
        
    except (LLMTimeoutError, LLMError) as e:
        return AIAdvancedChatResponse(
            response=f"âš ï¸ LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}\n\nvLLM ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
            mode=request.mode,
            tokens_used=0,
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": str(e), "code": "INTERNAL_ERROR"},
        )
