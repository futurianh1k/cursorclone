"""
AI ÎùºÏö∞ÌÑ∞
- POST /api/ai/explain   - ÏΩîÎìú ÏÑ§Î™Ö
- POST /api/ai/chat      - ÎåÄÌôîÌòï Ï±ÑÌåÖ
- POST /api/ai/rewrite   - ÏΩîÎìú ÏàòÏ†ï
- POST /api/ai/plan      - ÏûëÏóÖ Í≥ÑÌöç ÏàòÎ¶Ω
- POST /api/ai/agent     - ÏûêÎèô ÏΩîÎìú ÏûëÏÑ±/ÏàòÏ†ï
- POST /api/ai/debug     - Î≤ÑÍ∑∏ Î∂ÑÏÑù/ÏàòÏ†ï
"""

from fastapi import APIRouter, HTTPException, status
from ..models import (
    AIExplainRequest,
    AIExplainResponse,
    AIChatRequest,
    AIChatResponse,
    AIRewriteRequest,
    AIRewriteResponse,
    # AI Modes
    AIMode,
    AIPlanRequest,
    AIPlanResponse,
    TaskStep,
    AIAgentRequest,
    AIAgentResponse,
    FileChange,
    AIDebugRequest,
    AIDebugResponse,
    BugFix,
    ErrorResponse,
)
from ..context_builder import (
    DefaultContextBuilder,
    ContextBuildRequest,
    ContextSource,
    ContextSourceType,
    ActionType,
    SecurityError,
)
from ..llm import (
    get_llm_client,
    LLMError,
    LLMTimeoutError,
)

router = APIRouter(prefix="/api/ai", tags=["ai"])

# Context Builder Ïù∏Ïä§ÌÑ¥Ïä§ (Ï†ÑÏó≠ ÎòêÎäî ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ)
# TODO: Ïã§Ï†úÎ°úÎäî ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÎòêÎäî ÏÑ§Ï†ïÏóêÏÑú Í∞ÄÏ†∏Ïò§Í∏∞
_context_builder_cache = {}


def _get_context_builder(workspace_id: str) -> DefaultContextBuilder:
    """
    ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§Î≥Ñ Context Builder Í∞ÄÏ†∏Ïò§Í∏∞
    
    TODO: Ïã§Ï†ú ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ Î£®Ìä∏ Í≤ΩÎ°ú Í∞ÄÏ†∏Ïò§Í∏∞
    """
    if workspace_id not in _context_builder_cache:
        # TODO: Ïã§Ï†ú ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ Î£®Ìä∏ Í≤ΩÎ°ú
        workspace_root = f"/workspaces/{workspace_id}"
        _context_builder_cache[workspace_id] = DefaultContextBuilder(
            workspace_root=workspace_root,
        )
    return _context_builder_cache[workspace_id]


def _validate_workspace_access(ws_id: str) -> bool:
    """ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ Ï†ëÍ∑º Í∂åÌïú Í≤ÄÏ¶ù"""
    # TODO: Ïã§Ï†ú Í∂åÌïú Í≤ÄÏ¶ù Î°úÏßÅ Íµ¨ÌòÑ
    return True


def _validate_path(path: str) -> bool:
    """Í≤ΩÎ°ú Í≤ÄÏ¶ù (ÌÉàÏ∂ú Î∞©ÏßÄ)"""
    if ".." in path:
        return False
    if path.startswith("/"):
        return False
    return True


async def _read_file_content(workspace_id: str, file_path: str) -> str:
    """
    ÌååÏùº ÎÇ¥Ïö© ÏùΩÍ∏∞ (files ÎùºÏö∞ÌÑ∞ÏôÄ ÎèôÏùºÌïú Î°úÏßÅ ÏÇ¨Ïö©)
    """
    import os
    from ..utils.filesystem import get_workspace_root, validate_path, read_file_content, workspace_exists
    
    workspace_root = get_workspace_root(workspace_id)
    
    # ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
    if not workspace_exists(workspace_root):
        raise ValueError(f"Workspace not found: {workspace_id}")
    
    # Í≤ΩÎ°ú Í≤ÄÏ¶ù Î∞è ÌååÏùº ÏùΩÍ∏∞
    try:
        full_path = validate_path(file_path, workspace_root)
        content, _ = read_file_content(full_path)
        return content
    except (ValueError, FileNotFoundError) as e:
        raise ValueError(f"Failed to read file: {e}")


@router.post(
    "/explain",
    response_model=AIExplainResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        401: {"model": ErrorResponse, "description": "Unauthorized"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        404: {"model": ErrorResponse, "description": "File not found"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ÏΩîÎìú ÏÑ§Î™Ö",
    description="ÏÑ†ÌÉùÎêú ÏΩîÎìúÏóê ÎåÄÌïú AI ÏÑ§Î™ÖÏùÑ Î∞òÌôòÌï©ÎãàÎã§.",
)
async def explain_code(request: AIExplainRequest):
    """
    ÏÑ†ÌÉùÎêú ÏΩîÎìúÏóê ÎåÄÌïú AI ÏÑ§Î™ÖÏùÑ Î∞òÌôòÌï©ÎãàÎã§.
    
    TODO: Ïã§Ï†ú AI ÏÑ§Î™Ö Íµ¨ÌòÑ
    - Context BuilderÎ•º ÌÜµÌï¥ Ïª®ÌÖçÏä§Ìä∏ Ï°∞Ìï©
    - vLLMÏúºÎ°ú ÏöîÏ≤≠ Ï†ÑÏÜ°
    - ÏùëÎãµ Ï≤òÎ¶¨ Î∞è Î∞òÌôò
    
    ÌùêÎ¶Ñ: API ‚Üí Context Builder ‚Üí vLLM ‚Üí ÏùëÎãµ
    
    ‚ö†Ô∏è Ï£ºÏùò (AGENTS.md Í∑úÏπô)
    - APIÎäî LLMÏùÑ ÏßÅÏ†ë Ìò∏Ï∂úÌïòÏßÄ ÏïäÏäµÎãàÎã§
    - Î∞òÎìúÏãú Context BuilderÎ•º Í≤ΩÏú†Ìï¥Ïïº Ìï©ÎãàÎã§
    """
    # Í≤ΩÎ°ú Í≤ÄÏ¶ù
    if not _validate_path(request.file_path):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Invalid path", "code": "AI_INVALID_PATH"},
        )
    
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    try:
        # ÌååÏùº ÎÇ¥Ïö© ÏùΩÍ∏∞
        file_content = await _read_file_content(request.workspace_id, request.file_path)
        
        # Context Builder Ï§ÄÎπÑ
        context_builder = _get_context_builder(request.workspace_id)
        
        # Ïª®ÌÖçÏä§Ìä∏ ÏÜåÏä§ Íµ¨ÏÑ±
        sources = [
            ContextSource(
                type=ContextSourceType.FILE if not request.selection else ContextSourceType.SELECTION,
                path=request.file_path,
                content=file_content,
                range=request.selection,
            )
        ]
        
        # Context Builder Ìò∏Ï∂ú
        context_request = ContextBuildRequest(
            workspace_id=request.workspace_id,
            action=ActionType.EXPLAIN,
            instruction=f"Îã§Ïùå ÏΩîÎìúÎ•º ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî: {request.file_path}",
            sources=sources,
        )
        
        context_response = await context_builder.build(context_request)
        
        # vLLM Ìò∏Ï∂ú (Í∞úÎ∞ú Î™®ÎìúÏóêÏÑúÎäî Mock ÏùëÎãµ)
        import os
        dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
        
        try:
            if dev_mode:
                # Í∞úÎ∞ú Î™®Îìú: Mock ÏùëÎãµ ÏÉùÏÑ±
                # ÏÑ†ÌÉùÎêú ÏΩîÎìú Ï∂îÏ∂ú
                if request.selection:
                    lines = file_content.split("\n")
                    start = request.selection.start_line - 1
                    end = request.selection.end_line
                    selected_code = "\n".join(lines[start:end])
                else:
                    selected_code = file_content[:500] + ("..." if len(file_content) > 500 else "")
                
                explanation = f"""## ÏΩîÎìú Î∂ÑÏÑù (Í∞úÎ∞ú Î™®Îìú)

**ÌååÏùº**: `{request.file_path}`

### ÏΩîÎìú ÎÇ¥Ïö©
```
{selected_code}
```

### ÏÑ§Î™Ö
Ïù¥ ÏΩîÎìúÎäî `{request.file_path}` ÌååÏùºÏùò ÎÇ¥Ïö©ÏûÖÎãàÎã§.

> ‚ö†Ô∏è **Í∞úÎ∞ú Î™®Îìú**: Ïã§Ï†ú LLM ÏÑúÎπÑÏä§Í∞Ä Ïó∞Í≤∞ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.
> vLLM ÏÑúÎ≤ÑÎ•º ÏãúÏûëÌïòÎ©¥ Ïã§Ï†ú AI Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§.

**vLLM ÏÑ§Ï†ï Î∞©Î≤ï**:
```bash
# docker-compose.ymlÏóê vLLM ÏÑúÎπÑÏä§ Ï∂îÍ∞Ä ÎòêÎäî
# VLLM_BASE_URL ÌôòÍ≤ΩÎ≥ÄÏàò ÏÑ§Ï†ï
export VLLM_BASE_URL=http://your-vllm-server:8000/v1
```
"""
                return AIExplainResponse(
                    explanation=explanation,
                    tokensUsed=0,
                )
            
            # ÌîÑÎ°úÎçïÏÖò Î™®Îìú: Ïã§Ï†ú LLM Ìò∏Ï∂ú
            llm_client = get_llm_client()
            
            # Î©îÏãúÏßÄ ÌòïÏãù Î≥ÄÌôò
            messages = [
                {"role": msg.role, "content": msg.content}
                for msg in context_response.messages
            ]
            
            llm_response = await llm_client.chat(
                messages=messages,
                max_tokens=request.max_tokens if hasattr(request, "max_tokens") else None,
            )
            
            # ÏùëÎãµ Ï∂îÏ∂ú
            explanation = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            if not explanation:
                explanation = "[LLM ÏùëÎãµÏù¥ ÎπÑÏñ¥ÏûàÏäµÎãàÎã§]"
            
            return AIExplainResponse(
                explanation=explanation,
                tokensUsed=tokens_used,
            )
            
        except LLMTimeoutError as e:
            raise HTTPException(
                status_code=status.HTTP_504_GATEWAY_TIMEOUT,
                detail={"error": "LLM timeout", "code": "LLM_TIMEOUT", "detail": str(e)},
            )
        except LLMError as e:
            # LLM ÏóêÎü¨ Ïãú Í∞úÎ∞ú Î™®Îìú ÏùëÎãµ Ï†úÍ≥µ
            explanation = f"""## LLM ÏÑúÎπÑÏä§ Ïó∞Í≤∞ Ïã§Ìå®

**ÏóêÎü¨**: {str(e)}

vLLM ÏÑúÎ≤ÑÍ∞Ä Ïã§ÌñâÎêòÍ≥† ÏûàÏßÄ ÏïäÍ±∞ÎÇò Ïó∞Í≤∞Ìï† Ïàò ÏóÜÏäµÎãàÎã§.

**Ìï¥Í≤∞ Î∞©Î≤ï**:
1. vLLM ÏÑúÎ≤Ñ ÏÉÅÌÉú ÌôïÏù∏
2. `VLLM_BASE_URL` ÌôòÍ≤ΩÎ≥ÄÏàò ÌôïÏù∏
3. ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ ÌôïÏù∏
"""
            return AIExplainResponse(
                explanation=explanation,
                tokensUsed=0,
            )
        
    except SecurityError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Security validation failed", "code": "SECURITY_ERROR", "detail": str(e)},
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR"},
        )


@router.post(
    "/chat",
    response_model=AIChatResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        401: {"model": ErrorResponse, "description": "Unauthorized"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="AI Ï±ÑÌåÖ",
    description="ÏΩîÎìúÏóê ÎåÄÌïú ÏßàÎ¨∏, ÏàòÏ†ï ÏöîÏ≤≠, Ï£ºÏÑù Ï∂îÍ∞Ä Îì± ÏûêÏú†Î°úÏö¥ ÎåÄÌôîÌòï AI Ïù∏ÌÑ∞ÌéòÏù¥Ïä§.",
)
async def chat_with_ai(request: AIChatRequest):
    """
    ÎåÄÌôîÌòï AI Ïù∏ÌÑ∞ÌéòÏù¥Ïä§.
    
    - ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏ + ÌååÏùº Ïª®ÌÖçÏä§Ìä∏Î•º Ìï®Íªò LLMÏóê Ï†ÑÎã¨
    - "Ï£ºÏÑù Îã¨ÏïÑÏ§ò", "Ïù¥ ÏΩîÎìú Î≠êÏïº?", "Î≤ÑÍ∑∏ Ï∞æÏïÑÏ§ò" Îì± ÏûêÏú†Î°úÏö¥ ÏöîÏ≤≠ Ï≤òÎ¶¨
    - ÌååÏùºÏù¥ ÏóÜÏñ¥ÎèÑ ÏùºÎ∞ò ÏßàÎ¨∏ Í∞ÄÎä•
    
    ÌùêÎ¶Ñ: API ‚Üí Context Builder ‚Üí vLLM ‚Üí ÏùëÎãµ
    """
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    # ÌååÏùº Í≤ΩÎ°ú Í≤ÄÏ¶ù
    if request.file_path and not _validate_path(request.file_path):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Invalid path", "code": "AI_INVALID_PATH"},
        )
    
    try:
        import os
        dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
        
        # ÌååÏùº ÎÇ¥Ïö© Í∞ÄÏ†∏Ïò§Í∏∞
        file_content = None
        if request.file_path:
            try:
                if request.file_content:
                    file_content = request.file_content
                else:
                    file_content = await _read_file_content(request.workspace_id, request.file_path)
            except Exception as e:
                # ÌååÏùº ÏùΩÍ∏∞ Ïã§Ìå®Ìï¥ÎèÑ Ï±ÑÌåÖÏùÄ Í≥ÑÏÜç
                file_content = f"[ÌååÏùº ÏùΩÍ∏∞ Ïã§Ìå®: {e}]"
        
        # ÏÑ†ÌÉùÎêú ÏΩîÎìú Ï∂îÏ∂ú
        selected_code = None
        if file_content and request.selection:
            lines = file_content.split("\n")
            start = request.selection.start_line - 1
            end = request.selection.end_line
            selected_code = "\n".join(lines[start:end])
        
        if dev_mode:
            # Í∞úÎ∞ú Î™®Îìú: Mock ÏùëÎãµ ÏÉùÏÑ±
            response_text = _generate_mock_chat_response(
                message=request.message,
                file_path=request.file_path,
                file_content=file_content,
                selected_code=selected_code,
            )
            
            return AIChatResponse(
                response=response_text,
                tokensUsed=0,
                suggestedAction=_detect_action(request.message),
            )
        
        # ÌîÑÎ°úÎçïÏÖò Î™®Îìú: Ïã§Ï†ú LLM Ìò∏Ï∂ú
        try:
            llm_client = get_llm_client()
            
            # ÌååÏùºÏù¥ ÏûàÏúºÎ©¥ Context Builder ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ ÏßÅÏ†ë Ìò∏Ï∂ú
            if file_content:
                context_builder = _get_context_builder(request.workspace_id)
                
                sources = [
                    ContextSource(
                        type=ContextSourceType.SELECTION if selected_code else ContextSourceType.FILE,
                        path=request.file_path,
                        content=file_content,
                        range=request.selection,
                    )
                ]
                
                context_request = ContextBuildRequest(
                    workspace_id=request.workspace_id,
                    action=ActionType.EXPLAIN,
                    instruction=request.message,
                    sources=sources,
                )
                
                context_response = await context_builder.build(context_request)
                messages = [
                    {"role": msg.role, "content": msg.content}
                    for msg in context_response.messages
                ]
            else:
                # ÌååÏùº ÏóÜÏù¥ ÏùºÎ∞ò Ï±ÑÌåÖ
                messages = [
                    {"role": "system", "content": "You are a helpful coding assistant. Answer questions about programming, software development, and technology. Respond in the same language as the user's question."},
                    {"role": "user", "content": request.message},
                ]
            
            llm_response = await llm_client.chat(messages=messages)
            
            response_text = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            return AIChatResponse(
                response=response_text or "[ÏùëÎãµÏù¥ ÎπÑÏñ¥ÏûàÏäµÎãàÎã§]",
                tokensUsed=tokens_used,
                suggestedAction=_detect_action(request.message),
            )
            
        except (LLMTimeoutError, LLMError) as e:
            # LLM ÏóêÎü¨ Ïãú ÏπúÌôîÏ†ÅÏù∏ ÏùëÎãµ
            return AIChatResponse(
                response=f"‚ö†Ô∏è LLM ÏÑúÎπÑÏä§Ïóê Ïó∞Í≤∞Ìï† Ïàò ÏóÜÏäµÎãàÎã§.\n\n**ÏóêÎü¨**: {str(e)}\n\n`VLLM_BASE_URL` ÌôòÍ≤ΩÎ≥ÄÏàòÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.",
                tokensUsed=0,
            )
            
    except SecurityError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Security validation failed", "code": "SECURITY_ERROR", "detail": str(e)},
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR", "detail": str(e)},
        )


def _detect_action(message: str) -> str | None:
    """ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄÏóêÏÑú Ï†úÏïà Ïï°ÏÖò Í∞êÏßÄ"""
    message_lower = message.lower()
    
    rewrite_keywords = ["ÏàòÏ†ï", "Î≥ÄÍ≤Ω", "Î∞îÍøî", "Ï∂îÍ∞Ä", "ÏÇ≠Ï†ú", "Ï£ºÏÑù", "Î¶¨Ìå©ÌÑ∞", "fix", "change", "add", "remove", "comment"]
    explain_keywords = ["ÏÑ§Î™Ö", "Î≠êÏïº", "Î≠îÍ∞ÄÏöî", "ÏïåÎ†§Ï§ò", "Ïñ¥ÎñªÍ≤å", "explain", "what", "how", "why"]
    
    if any(kw in message_lower for kw in rewrite_keywords):
        return "rewrite"
    if any(kw in message_lower for kw in explain_keywords):
        return "explain"
    
    return None


def _generate_mock_chat_response(
    message: str,
    file_path: str | None,
    file_content: str | None,
    selected_code: str | None,
) -> str:
    """Í∞úÎ∞ú Î™®ÎìúÏö© Mock ÏùëÎãµ ÏÉùÏÑ±"""
    # ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ Î∂ÑÏÑù
    action = _detect_action(message)
    
    if not file_path:
        return f"""ÏïàÎÖïÌïòÏÑ∏Ïöî! Ï†ÄÎäî ÏΩîÎìú Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§.

**ÏßàÎ¨∏**: {message}

ÌòÑÏû¨ ÌååÏùºÏù¥ ÏÑ†ÌÉùÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. ÌååÏùºÏùÑ Ïó¥Í≥† ÏΩîÎìúÏóê ÎåÄÌï¥ ÏßàÎ¨∏ÌïòÏãúÎ©¥ Îçî Ï†ïÌôïÌïú ÎèÑÏõÄÏùÑ ÎìúÎ¶¥ Ïàò ÏûàÏäµÎãàÎã§.

---
> ‚ö†Ô∏è **Í∞úÎ∞ú Î™®Îìú**: vLLM ÏÑúÎ≤ÑÍ∞Ä Ïó∞Í≤∞ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.
"""
    
    code_preview = selected_code or (file_content[:300] + "..." if file_content and len(file_content) > 300 else file_content)
    
    if action == "rewrite" and "Ï£ºÏÑù" in message:
        # Ï£ºÏÑù Ï∂îÍ∞Ä ÏöîÏ≤≠Ïóê ÎåÄÌïú Mock ÏùëÎãµ
        return f"""## Ï£ºÏÑù Ï∂îÍ∞Ä Ï†úÏïà

**ÌååÏùº**: `{file_path}`

**ÏöîÏ≤≠**: {message}

### ÏõêÎ≥∏ ÏΩîÎìú
```
{code_preview}
```

### Ï†úÏïàÎêú Î≥ÄÍ≤ΩÏÇ¨Ìï≠

ÏΩîÎìúÏóê Ï£ºÏÑùÏùÑ Ï∂îÍ∞ÄÌïòÎ†§Î©¥ **Rewrite Î™®Îìú**Î•º ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî:
1. Ï£ºÏÑùÏùÑ Îã¨ ÏΩîÎìú ÏòÅÏó≠ÏùÑ ÏÑ†ÌÉù
2. "Rewrite" Î™®ÎìúÎ°ú Ï†ÑÌôò
3. "Ï£ºÏÑù Îã¨ÏïÑÏ§ò"ÎùºÍ≥† ÏûÖÎ†•

---
> ‚ö†Ô∏è **Í∞úÎ∞ú Î™®Îìú**: Ïã§Ï†ú ÏΩîÎìú ÏàòÏ†ïÏùÄ vLLM Ïó∞Í≤∞ ÌõÑ Í∞ÄÎä•Ìï©ÎãàÎã§.
> `VLLM_BASE_URL` ÌôòÍ≤ΩÎ≥ÄÏàòÎ•º ÏÑ§Ï†ïÌïòÏÑ∏Ïöî.
"""
    
    return f"""## AI ÏùëÎãµ

**ÌååÏùº**: `{file_path}`

**ÏßàÎ¨∏**: {message}

### ÏΩîÎìú ÎÇ¥Ïö©
```
{code_preview}
```

### Î∂ÑÏÑù

Ïù¥ ÏΩîÎìúÎäî `{file_path}` ÌååÏùºÏùò ÎÇ¥Ïö©ÏûÖÎãàÎã§.

{f"ÏÑ†ÌÉùÎêú ÏòÅÏó≠ ({len(selected_code.split(chr(10)))}Ï§Ñ)ÏùÑ Î∂ÑÏÑùÌñàÏäµÎãàÎã§." if selected_code else "Ï†ÑÏ≤¥ ÌååÏùºÏùÑ Î∂ÑÏÑùÌñàÏäµÎãàÎã§."}

---
> ‚ö†Ô∏è **Í∞úÎ∞ú Î™®Îìú**: Ïã§Ï†ú AI Î∂ÑÏÑùÏùÄ vLLM ÏÑúÎ≤Ñ Ïó∞Í≤∞ ÌõÑ Í∞ÄÎä•Ìï©ÎãàÎã§.
"""


@router.post(
    "/rewrite",
    response_model=AIRewriteResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        401: {"model": ErrorResponse, "description": "Unauthorized"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        404: {"model": ErrorResponse, "description": "File not found"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ÏΩîÎìú Î¶¨ÎùºÏù¥Ìä∏",
    description="ÏßÄÏãúÏÇ¨Ìï≠Ïóê Îî∞Îùº ÏΩîÎìúÎ•º ÏàòÏ†ïÌïòÍ≥† unified diffÎ•º Î∞òÌôòÌï©ÎãàÎã§.",
)
async def rewrite_code(request: AIRewriteRequest):
    """
    ÏßÄÏãúÏÇ¨Ìï≠Ïóê Îî∞Îùº ÏΩîÎìúÎ•º ÏàòÏ†ïÌïòÍ≥† unified diffÎ•º Î∞òÌôòÌï©ÎãàÎã§.
    
    ‚ö†Ô∏è Ï§ëÏöî: Ïù¥ APIÎäî diffÎßå Î∞òÌôòÌï©ÎãàÎã§.
    Ïã§Ï†ú Ï†ÅÏö©ÏùÄ /patch/validate ‚Üí /patch/applyÎ•º ÌÜµÌï¥ ÏàòÌñâÌï¥Ïïº Ìï©ÎãàÎã§.
    
    TODO: Ïã§Ï†ú AI Î¶¨ÎùºÏù¥Ìä∏ Íµ¨ÌòÑ
    - Context BuilderÎ•º ÌÜµÌï¥ Ïª®ÌÖçÏä§Ìä∏ Ï°∞Ìï©
    - vLLMÏúºÎ°ú ÏöîÏ≤≠ Ï†ÑÏÜ° (rewrite ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø ÏÇ¨Ïö©)
    - unified diff ÌòïÏãù ÏùëÎãµ ÌååÏã±
    
    ÌùêÎ¶Ñ: API ‚Üí Context Builder ‚Üí vLLM ‚Üí diff Î∞òÌôò
          ‚Üí (ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏) ‚Üí /patch/validate ‚Üí /patch/apply
    
    ‚ö†Ô∏è Ï£ºÏùò (AGENTS.md Í∑úÏπô)
    - APIÎäî LLMÏùÑ ÏßÅÏ†ë Ìò∏Ï∂úÌïòÏßÄ ÏïäÏäµÎãàÎã§
    - ÏΩîÎìú Î≥ÄÍ≤ΩÏùÄ Î∞òÎìúÏãú Patch Í≤ΩÎ°úÎ°ú Ï†ÅÏö©Ìï¥Ïïº Ìï©ÎãàÎã§
    """
    target_file = request.target.get("file", "")
    
    # Í≤ΩÎ°ú Í≤ÄÏ¶ù
    if not _validate_path(target_file):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Invalid path", "code": "AI_INVALID_PATH"},
        )
    
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    try:
        # ÌååÏùº ÎÇ¥Ïö© ÏùΩÍ∏∞
        file_content = await _read_file_content(request.workspace_id, target_file)
        
        # ÏÑ†ÌÉù Î≤îÏúÑ Ï∂îÏ∂ú
        selection_range = None
        if "selection" in request.target:
            from ..context_builder import SelectionRange
            sel = request.target["selection"]
            selection_range = SelectionRange(
                start_line=sel.get("startLine", sel.get("start_line", 1)),
                end_line=sel.get("endLine", sel.get("end_line", 1)),
            )
        
        # Context Builder Ï§ÄÎπÑ
        context_builder = _get_context_builder(request.workspace_id)
        
        # Ïª®ÌÖçÏä§Ìä∏ ÏÜåÏä§ Íµ¨ÏÑ±
        sources = [
            ContextSource(
                type=ContextSourceType.SELECTION if selection_range else ContextSourceType.FILE,
                path=target_file,
                content=file_content,
                range=selection_range,
            )
        ]
        
        # Context Builder Ìò∏Ï∂ú
        context_request = ContextBuildRequest(
            workspace_id=request.workspace_id,
            action=ActionType.REWRITE,
            instruction=request.instruction,
            sources=sources,
        )
        
        context_response = await context_builder.build(context_request)
        
        # vLLM Ìò∏Ï∂ú (rewrite ÌÖúÌîåÎ¶ø ÏÇ¨Ïö©)
        try:
            llm_client = get_llm_client()
            
            # Î©îÏãúÏßÄ ÌòïÏãù Î≥ÄÌôò
            messages = [
                {"role": msg.role, "content": msg.content}
                for msg in context_response.messages
            ]
            
            llm_response = await llm_client.chat(
                messages=messages,
                max_tokens=request.max_tokens if hasattr(request, "max_tokens") else None,
            )
            
            # ÏùëÎãµ Ï∂îÏ∂ú (unified diff ÌòïÏãù)
            diff = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            if not diff:
                # Îπà diffÏù∏ Í≤ΩÏö∞ Í∏∞Î≥∏ ÌòïÏãù Î∞òÌôò
                diff = f"""--- a/{target_file}
+++ b/{target_file}
@@ -1,0 +1,0 @@
"""
            
            return AIRewriteResponse(
                diff=diff,
                tokensUsed=tokens_used,
            )
            
        except LLMTimeoutError as e:
            raise HTTPException(
                status_code=status.HTTP_504_GATEWAY_TIMEOUT,
                detail={"error": "LLM timeout", "code": "LLM_TIMEOUT", "detail": str(e)},
            )
        except LLMError as e:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail={"error": "LLM service error", "code": "LLM_ERROR", "detail": str(e)},
            )
        
    except SecurityError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Security validation failed", "code": "SECURITY_ERROR", "detail": str(e)},
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR"},
        )


# ============================================================
# AI Plan Mode - ÏûëÏóÖ Í≥ÑÌöç ÏàòÎ¶Ω
# ============================================================

@router.post(
    "/plan",
    response_model=AIPlanResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ÏûëÏóÖ Í≥ÑÌöç ÏàòÎ¶Ω (Plan Î™®Îìú)",
    description="Î™©ÌëúÎ•º Î∂ÑÏÑùÌïòÍ≥† Îã®Í≥ÑÎ≥Ñ Ïã§Ìñâ Í≥ÑÌöçÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.",
)
async def create_plan(request: AIPlanRequest):
    """
    ÏÇ¨Ïö©ÏûêÍ∞Ä Ï†úÏãúÌïú Î™©ÌëúÎ•º Î∂ÑÏÑùÌïòÍ≥†, Ïù¥Î•º Îã¨ÏÑ±ÌïòÍ∏∞ ÏúÑÌïú 
    Îã®Í≥ÑÎ≥Ñ Ïã§Ìñâ Í≥ÑÌöçÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.
    
    ÏòàÏãú:
    - "Î°úÍ∑∏Ïù∏ Í∏∞Îä• Ï∂îÍ∞Ä"
    - "ÌÖåÏä§Ìä∏ ÏΩîÎìú ÏûëÏÑ±"
    - "ÏΩîÎìú Î¶¨Ìå©ÌÜ†ÎßÅ"
    """
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    try:
        import os
        dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
        
        # Í¥ÄÎ†® ÌååÏùº ÎÇ¥Ïö© ÏàòÏßë
        file_contents = {}
        if request.file_paths:
            for fp in request.file_paths[:5]:  # ÏµúÎåÄ 5Í∞ú ÌååÏùº
                if _validate_path(fp):
                    try:
                        content = await _read_file_content(request.workspace_id, fp)
                        file_contents[fp] = content
                    except:
                        pass
        
        if dev_mode:
            # Í∞úÎ∞ú Î™®Îìú: Mock Í≥ÑÌöç ÏÉùÏÑ±
            steps = _generate_mock_plan_steps(request.goal, file_contents)
            return AIPlanResponse(
                summary=f"'{request.goal}'Î•º ÏúÑÌïú Ïã§Ìñâ Í≥ÑÌöçÏûÖÎãàÎã§. (Í∞úÎ∞ú Î™®Îìú)",
                steps=steps,
                estimatedChanges=len(steps),
                tokensUsed=0,
            )
        
        # ÌîÑÎ°úÎçïÏÖò Î™®Îìú: Ïã§Ï†ú LLM Ìò∏Ï∂ú
        try:
            llm_client = get_llm_client()
            
            # Plan ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
            files_context = ""
            if file_contents:
                files_context = "\n\nÍ¥ÄÎ†® ÌååÏùº:\n" + "\n".join([
                    f"### {fp}\n```\n{content[:500]}...\n```" 
                    for fp, content in file_contents.items()
                ])
            
            messages = [
                {"role": "system", "content": """You are a planning assistant. Given a goal, create a detailed step-by-step plan.

Output format (JSON):
{
  "summary": "Brief summary of the plan",
  "steps": [
    {"stepNumber": 1, "description": "Step description", "filePath": "optional/file/path.py"},
    ...
  ]
}

Keep the plan focused and actionable. Each step should be clear and specific."""},
                {"role": "user", "content": f"Goal: {request.goal}\n\nContext: {request.context or 'None'}{files_context}"}
            ]
            
            llm_response = await llm_client.chat(messages=messages)
            response_text = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            # JSON ÌååÏã± ÏãúÎèÑ
            try:
                import json
                # JSON Î∏îÎ°ù Ï∂îÏ∂ú
                if "```json" in response_text:
                    json_str = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    json_str = response_text.split("```")[1].split("```")[0]
                else:
                    json_str = response_text
                    
                plan_data = json.loads(json_str.strip())
                steps = [
                    TaskStep(
                        stepNumber=s.get("stepNumber", i+1),
                        description=s.get("description", ""),
                        filePath=s.get("filePath"),
                    )
                    for i, s in enumerate(plan_data.get("steps", []))
                ]
                summary = plan_data.get("summary", f"Plan for: {request.goal}")
            except:
                # ÌååÏã± Ïã§Ìå®Ïãú Í∏∞Î≥∏ ÏùëÎãµ
                steps = [TaskStep(stepNumber=1, description=response_text[:500])]
                summary = f"Plan for: {request.goal}"
            
            return AIPlanResponse(
                summary=summary,
                steps=steps,
                estimatedChanges=len(steps),
                tokensUsed=tokens_used,
            )
            
        except (LLMTimeoutError, LLMError) as e:
            # LLM Ïò§Î•òÏãú Í∏∞Î≥∏ Í≥ÑÌöç Î∞òÌôò
            return AIPlanResponse(
                summary=f"‚ö†Ô∏è LLM Ïó∞Í≤∞ Ïã§Ìå®: {str(e)}",
                steps=[TaskStep(stepNumber=1, description="LLM ÏÑúÎ≤Ñ Ïó∞Í≤∞ ÌôïÏù∏ ÌïÑÏöî")],
                estimatedChanges=0,
                tokensUsed=0,
            )
            
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR", "detail": str(e)},
        )


def _generate_mock_plan_steps(goal: str, file_contents: dict) -> list:
    """Í∞úÎ∞ú Î™®ÎìúÏö© Mock Í≥ÑÌöç Îã®Í≥Ñ ÏÉùÏÑ±"""
    goal_lower = goal.lower()
    
    if "Î°úÍ∑∏Ïù∏" in goal_lower or "auth" in goal_lower:
        return [
            TaskStep(stepNumber=1, description="ÏÇ¨Ïö©Ïûê Î™®Îç∏ Î∞è DB Ïä§ÌÇ§Îßà Ï†ïÏùò", filePath="src/models/user.py"),
            TaskStep(stepNumber=2, description="ÎπÑÎ∞ÄÎ≤àÌò∏ Ìï¥Ïã± Ïú†Ìã∏Î¶¨Ìã∞ Íµ¨ÌòÑ", filePath="src/utils/auth.py"),
            TaskStep(stepNumber=3, description="Î°úÍ∑∏Ïù∏ API ÏóîÎìúÌè¨Ïù∏Ìä∏ ÏÉùÏÑ±", filePath="src/routers/auth.py"),
            TaskStep(stepNumber=4, description="JWT ÌÜ†ÌÅ∞ Î∞úÍ∏â Î°úÏßÅ Íµ¨ÌòÑ", filePath="src/services/jwt.py"),
            TaskStep(stepNumber=5, description="Î°úÍ∑∏Ïù∏ ÌÖåÏä§Ìä∏ ÏûëÏÑ±", filePath="tests/test_auth.py"),
        ]
    elif "ÌÖåÏä§Ìä∏" in goal_lower or "test" in goal_lower:
        return [
            TaskStep(stepNumber=1, description="ÌÖåÏä§Ìä∏ ÌôòÍ≤Ω ÏÑ§Ï†ï (pytest)", filePath="pytest.ini"),
            TaskStep(stepNumber=2, description="Ïú†Îãõ ÌÖåÏä§Ìä∏ ÏûëÏÑ±", filePath="tests/test_unit.py"),
            TaskStep(stepNumber=3, description="ÌÜµÌï© ÌÖåÏä§Ìä∏ ÏûëÏÑ±", filePath="tests/test_integration.py"),
            TaskStep(stepNumber=4, description="ÌÖåÏä§Ìä∏ Ïª§Î≤ÑÎ¶¨ÏßÄ ÌôïÏù∏"),
        ]
    elif "Î¶¨Ìå©ÌÜ†ÎßÅ" in goal_lower or "refactor" in goal_lower:
        return [
            TaskStep(stepNumber=1, description="Ï§ëÎ≥µ ÏΩîÎìú ÏãùÎ≥Ñ Î∞è Î∂ÑÏÑù"),
            TaskStep(stepNumber=2, description="Í≥µÌÜµ Ïú†Ìã∏Î¶¨Ìã∞ Ìï®Ïàò Ï∂îÏ∂ú", filePath="src/utils/common.py"),
            TaskStep(stepNumber=3, description="Í∏∞Ï°¥ ÏΩîÎìúÏóêÏÑú Ïú†Ìã∏Î¶¨Ìã∞ ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÏàòÏ†ï"),
            TaskStep(stepNumber=4, description="ÌÖåÏä§Ìä∏ Ïã§Ìñâ Î∞è ÌôïÏù∏"),
        ]
    else:
        # ÏùºÎ∞òÏ†ÅÏù∏ Í≥ÑÌöç
        return [
            TaskStep(stepNumber=1, description=f"'{goal}' ÏöîÍµ¨ÏÇ¨Ìï≠ Î∂ÑÏÑù"),
            TaskStep(stepNumber=2, description="ÌïÑÏöîÌïú ÌååÏùº/Î™®Îìà ÏãùÎ≥Ñ"),
            TaskStep(stepNumber=3, description="ÏΩîÎìú Íµ¨ÌòÑ"),
            TaskStep(stepNumber=4, description="ÌÖåÏä§Ìä∏ ÏûëÏÑ± Î∞è Ïã§Ìñâ"),
            TaskStep(stepNumber=5, description="Î¨∏ÏÑúÌôî"),
        ]


# ============================================================
# AI Agent Mode - ÏûêÎèô ÏΩîÎìú ÏûëÏÑ±/ÏàòÏ†ï
# ============================================================

@router.post(
    "/agent",
    response_model=AIAgentResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ÏûêÎèô ÏΩîÎìú ÏûëÏÑ±/ÏàòÏ†ï (Agent Î™®Îìú)",
    description="ÏßÄÏãúÏÇ¨Ìï≠Ïóê Îî∞Îùº ÏûêÎèôÏúºÎ°ú ÏΩîÎìúÎ•º Î∂ÑÏÑùÌïòÍ≥† Î≥ÄÍ≤Ω ÏÇ¨Ìï≠ÏùÑ Ï†úÏïàÌï©ÎãàÎã§.",
)
async def run_agent(request: AIAgentRequest):
    """
    ÏóêÏù¥Ï†ÑÌä∏ Î™®Îìú: ÏÇ¨Ïö©Ïûê ÏßÄÏãúÏóê Îî∞Îùº ÏûêÎèôÏúºÎ°ú ÏΩîÎìúÎ•º Î∂ÑÏÑùÌïòÍ≥†,
    ÌïÑÏöîÌïú ÌååÏùºÏùÑ ÏàòÏ†ï/ÏÉùÏÑ±/ÏÇ≠Ï†úÌïòÎäî Î≥ÄÍ≤Ω ÏÇ¨Ìï≠ÏùÑ Ï†úÏïàÌï©ÎãàÎã§.
    
    auto_apply=TrueÏù∏ Í≤ΩÏö∞ Î≥ÄÍ≤Ω ÏÇ¨Ìï≠ÏùÑ ÏßÅÏ†ë Ï†ÅÏö©Ìï©ÎãàÎã§.
    (‚ö†Ô∏è Ï£ºÏùò: ÌòÑÏû¨ PoCÏóêÏÑúÎäî auto_applyÎäî Î¨¥ÏãúÎê©ÎãàÎã§)
    """
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    try:
        import os
        dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
        
        # Í¥ÄÎ†® ÌååÏùº ÎÇ¥Ïö© ÏàòÏßë
        file_contents = {}
        if request.file_paths:
            for fp in request.file_paths[:10]:  # ÏµúÎåÄ 10Í∞ú ÌååÏùº
                if _validate_path(fp):
                    try:
                        content = await _read_file_content(request.workspace_id, fp)
                        file_contents[fp] = content
                    except:
                        pass
        
        if dev_mode:
            # Í∞úÎ∞ú Î™®Îìú: Mock ÏùëÎãµ ÏÉùÏÑ±
            changes = _generate_mock_agent_changes(request.instruction, file_contents)
            return AIAgentResponse(
                summary=f"'{request.instruction}' ÏûëÏóÖ ÏôÑÎ£å (Í∞úÎ∞ú Î™®Îìú)",
                changes=changes,
                applied=False,
                tokensUsed=0,
            )
        
        # ÌîÑÎ°úÎçïÏÖò Î™®Îìú: Ïã§Ï†ú LLM Ìò∏Ï∂ú
        try:
            llm_client = get_llm_client()
            
            # Agent ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
            files_context = ""
            if file_contents:
                files_context = "\n\nFiles to modify:\n" + "\n".join([
                    f"### {fp}\n```\n{content}\n```" 
                    for fp, content in file_contents.items()
                ])
            
            messages = [
                {"role": "system", "content": """You are a coding agent. Given an instruction, analyze the code and provide specific changes.

Output format (JSON):
{
  "summary": "What was done",
  "changes": [
    {
      "filePath": "path/to/file.py",
      "action": "modify|create|delete",
      "diff": "--- a/file.py\\n+++ b/file.py\\n@@ -1,3 +1,4 @@\\n...",
      "description": "What this change does"
    }
  ]
}

Be specific and provide actual code changes in unified diff format."""},
                {"role": "user", "content": f"Instruction: {request.instruction}{files_context}"}
            ]
            
            llm_response = await llm_client.chat(messages=messages)
            response_text = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            # JSON ÌååÏã± ÏãúÎèÑ
            try:
                import json
                if "```json" in response_text:
                    json_str = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    json_str = response_text.split("```")[1].split("```")[0]
                else:
                    json_str = response_text
                    
                agent_data = json.loads(json_str.strip())
                changes = [
                    FileChange(
                        filePath=c.get("filePath", "unknown"),
                        action=c.get("action", "modify"),
                        diff=c.get("diff"),
                        description=c.get("description", ""),
                    )
                    for c in agent_data.get("changes", [])
                ]
                summary = agent_data.get("summary", "Changes generated")
            except:
                changes = [FileChange(
                    filePath="unknown",
                    action="modify",
                    description=response_text[:500],
                )]
                summary = "Agent response (parsing failed)"
            
            return AIAgentResponse(
                summary=summary,
                changes=changes,
                applied=False,  # PoCÏóêÏÑúÎäî ÏûêÎèô Ï†ÅÏö© ÎπÑÌôúÏÑ±Ìôî
                tokensUsed=tokens_used,
            )
            
        except (LLMTimeoutError, LLMError) as e:
            return AIAgentResponse(
                summary=f"‚ö†Ô∏è LLM Ïó∞Í≤∞ Ïã§Ìå®: {str(e)}",
                changes=[],
                applied=False,
                tokensUsed=0,
            )
            
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR", "detail": str(e)},
        )


def _generate_mock_agent_changes(instruction: str, file_contents: dict) -> list:
    """Í∞úÎ∞ú Î™®ÎìúÏö© Mock Agent Î≥ÄÍ≤Ω ÏÇ¨Ìï≠ ÏÉùÏÑ±"""
    changes = []
    
    if "Ï£ºÏÑù" in instruction or "comment" in instruction.lower():
        for fp, content in file_contents.items():
            lines = content.split("\n")
            diff_lines = [f"--- a/{fp}", f"+++ b/{fp}", "@@ -1,3 +1,4 @@"]
            if lines:
                diff_lines.append(f"+# {instruction}")
                diff_lines.append(f" {lines[0]}")
            
            changes.append(FileChange(
                filePath=fp,
                action="modify",
                diff="\n".join(diff_lines),
                description=f"{fp}Ïóê Ï£ºÏÑù Ï∂îÍ∞Ä",
            ))
    elif "ÏÉùÏÑ±" in instruction or "create" in instruction.lower():
        changes.append(FileChange(
            filePath="new_file.py",
            action="create",
            diff="""--- /dev/null
+++ b/new_file.py
@@ -0,0 +1,5 @@
+\"\"\"
+Auto-generated file
+\"\"\"
+
+# TODO: Implement
""",
            description="ÏÉà ÌååÏùº ÏÉùÏÑ±",
        ))
    else:
        # Í∏∞Ï°¥ ÌååÏùº ÏàòÏ†ï Ï†úÏïà
        for fp in list(file_contents.keys())[:2]:
            changes.append(FileChange(
                filePath=fp,
                action="modify",
                description=f"'{instruction}'Ïóê Îî∞Îùº {fp} ÏàòÏ†ï ÌïÑÏöî (Í∞úÎ∞ú Î™®Îìú)",
            ))
    
    if not changes:
        changes.append(FileChange(
            filePath="example.py",
            action="modify",
            description=f"'{instruction}' ÏûëÏóÖÏùÑ ÏúÑÌïú Î≥ÄÍ≤Ω ÏÇ¨Ìï≠ (Í∞úÎ∞ú Î™®Îìú)",
        ))
    
    return changes


# ============================================================
# AI Debug Mode - Î≤ÑÍ∑∏ Î∂ÑÏÑù/ÏàòÏ†ï
# ============================================================

@router.post(
    "/debug",
    response_model=AIDebugResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="Î≤ÑÍ∑∏ Î∂ÑÏÑù/ÏàòÏ†ï (Debug Î™®Îìú)",
    description="ÏóêÎü¨ Î©îÏãúÏßÄ, Ïä§ÌÉù Ìä∏Î†àÏù¥Ïä§, ÏΩîÎìúÎ•º Î∂ÑÏÑùÌïòÏó¨ Î≤ÑÍ∑∏ ÏõêÏù∏Í≥º Ìï¥Í≤∞Ï±ÖÏùÑ Ï†úÏãúÌï©ÎãàÎã§.",
)
async def debug_code(request: AIDebugRequest):
    """
    ÎîîÎ≤ÑÍ∑∏ Î™®Îìú: ÏóêÎü¨ Î©îÏãúÏßÄ, Ïä§ÌÉù Ìä∏Î†àÏù¥Ïä§, Í¥ÄÎ†® ÏΩîÎìúÎ•º Î∂ÑÏÑùÌïòÏó¨
    Î≤ÑÍ∑∏Ïùò ÏõêÏù∏ÏùÑ ÏßÑÎã®ÌïòÍ≥† ÏàòÏ†ï Î∞©ÏïàÏùÑ Ï†úÏãúÌï©ÎãàÎã§.
    """
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    if not request.error_message and not request.stack_trace and not request.description:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "At least one of error_message, stack_trace, or description is required", "code": "DEBUG_NO_INPUT"},
        )
    
    try:
        import os
        dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
        
        # ÌååÏùº ÎÇ¥Ïö© Í∞ÄÏ†∏Ïò§Í∏∞
        file_content = None
        if request.file_path and _validate_path(request.file_path):
            try:
                if request.file_content:
                    file_content = request.file_content
                else:
                    file_content = await _read_file_content(request.workspace_id, request.file_path)
            except:
                pass
        
        if dev_mode:
            # Í∞úÎ∞ú Î™®Îìú: Mock ÏßÑÎã® ÏÉùÏÑ±
            diagnosis, root_cause, fixes, tips = _generate_mock_debug_response(
                error_message=request.error_message,
                stack_trace=request.stack_trace,
                file_path=request.file_path,
                file_content=file_content,
                description=request.description,
            )
            return AIDebugResponse(
                diagnosis=diagnosis,
                rootCause=root_cause,
                fixes=fixes,
                preventionTips=tips,
                tokensUsed=0,
            )
        
        # ÌîÑÎ°úÎçïÏÖò Î™®Îìú: Ïã§Ï†ú LLM Ìò∏Ï∂ú
        try:
            llm_client = get_llm_client()
            
            # Debug ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
            debug_context = []
            if request.error_message:
                debug_context.append(f"Error Message:\n{request.error_message}")
            if request.stack_trace:
                debug_context.append(f"Stack Trace:\n{request.stack_trace}")
            if request.description:
                debug_context.append(f"Description:\n{request.description}")
            if file_content:
                debug_context.append(f"Code ({request.file_path}):\n```\n{file_content}\n```")
            
            messages = [
                {"role": "system", "content": """You are a debugging expert. Analyze the error and provide a diagnosis.

Output format (JSON):
{
  "diagnosis": "Detailed analysis of the problem",
  "rootCause": "The root cause of the bug",
  "fixes": [
    {
      "filePath": "path/to/file.py",
      "lineNumber": 42,
      "originalCode": "old code",
      "fixedCode": "new code",
      "explanation": "Why this fixes the issue"
    }
  ],
  "preventionTips": ["Tip 1", "Tip 2"]
}

Be thorough and specific in your analysis."""},
                {"role": "user", "content": "\n\n".join(debug_context)}
            ]
            
            llm_response = await llm_client.chat(messages=messages)
            response_text = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            # JSON ÌååÏã± ÏãúÎèÑ
            try:
                import json
                if "```json" in response_text:
                    json_str = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    json_str = response_text.split("```")[1].split("```")[0]
                else:
                    json_str = response_text
                    
                debug_data = json.loads(json_str.strip())
                fixes = [
                    BugFix(
                        filePath=f.get("filePath", request.file_path or "unknown"),
                        lineNumber=f.get("lineNumber"),
                        originalCode=f.get("originalCode", ""),
                        fixedCode=f.get("fixedCode", ""),
                        explanation=f.get("explanation", ""),
                    )
                    for f in debug_data.get("fixes", [])
                ]
                diagnosis = debug_data.get("diagnosis", "Analysis complete")
                root_cause = debug_data.get("rootCause", "Unknown")
                tips = debug_data.get("preventionTips", [])
            except:
                diagnosis = response_text[:500]
                root_cause = "LLM ÏùëÎãµ ÌååÏã± Ïã§Ìå®"
                fixes = []
                tips = []
            
            return AIDebugResponse(
                diagnosis=diagnosis,
                rootCause=root_cause,
                fixes=fixes,
                preventionTips=tips,
                tokensUsed=tokens_used,
            )
            
        except (LLMTimeoutError, LLMError) as e:
            return AIDebugResponse(
                diagnosis=f"‚ö†Ô∏è LLM Ïó∞Í≤∞ Ïã§Ìå®: {str(e)}",
                rootCause="LLM ÏÑúÎπÑÏä§ Ïó∞Í≤∞ ÌïÑÏöî",
                fixes=[],
                preventionTips=["vLLM ÏÑúÎ≤Ñ ÏÉÅÌÉú ÌôïÏù∏", "VLLM_BASE_URL ÌôòÍ≤ΩÎ≥ÄÏàò ÌôïÏù∏"],
                tokensUsed=0,
            )
            
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR", "detail": str(e)},
        )


def _generate_mock_debug_response(
    error_message: str | None,
    stack_trace: str | None,
    file_path: str | None,
    file_content: str | None,
    description: str | None,
) -> tuple:
    """Í∞úÎ∞ú Î™®ÎìúÏö© Mock ÎîîÎ≤ÑÍ∑∏ ÏùëÎãµ ÏÉùÏÑ±"""
    
    # ÏóêÎü¨ Ïú†Ìòï Î∂ÑÏÑù
    error_lower = (error_message or "").lower() + (stack_trace or "").lower()
    
    if "typeerror" in error_lower or "type" in error_lower:
        diagnosis = "TypeErrorÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Î≥ÄÏàòÏùò ÌÉÄÏûÖÏù¥ ÏòàÏÉÅÍ≥º Îã§Î¶ÖÎãàÎã§."
        root_cause = "Ìï®ÏàòÏóê ÏûòÎ™ªÎêú ÌÉÄÏûÖÏùò Ïù∏ÏûêÍ∞Ä Ï†ÑÎã¨ÎêòÏóàÍ±∞ÎÇò, None Í∞íÏóê ÎåÄÌï¥ Î©îÏÑúÎìúÎ•º Ìò∏Ï∂úÌñàÏäµÎãàÎã§."
        fixes = [BugFix(
            filePath=file_path or "unknown.py",
            lineNumber=1,
            originalCode="result = data.process()",
            fixedCode="result = data.process() if data is not None else None",
            explanation="None Ï≤¥ÌÅ¨Î•º Ï∂îÍ∞ÄÌïòÏó¨ TypeError Î∞©ÏßÄ",
        )]
        tips = ["ÌÉÄÏûÖ ÌûåÌä∏ ÏÇ¨Ïö©", "None Ï≤¥ÌÅ¨ Ï∂îÍ∞Ä", "isinstance()Î°ú ÌÉÄÏûÖ Í≤ÄÏ¶ù"]
        
    elif "importerror" in error_lower or "modulenotfound" in error_lower:
        diagnosis = "Î™®Îìà ÏûÑÌè¨Ìä∏ Ïò§Î•òÏûÖÎãàÎã§. ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄÍ∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÍ±∞ÎÇò Í≤ΩÎ°úÍ∞Ä ÏûòÎ™ªÎêòÏóàÏäµÎãàÎã§."
        root_cause = "Ìå®ÌÇ§ÏßÄÍ∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÍ±∞ÎÇò, Í∞ÄÏÉÅÌôòÍ≤ΩÏù¥ ÌôúÏÑ±ÌôîÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§."
        fixes = [BugFix(
            filePath="requirements.txt",
            lineNumber=None,
            originalCode="",
            fixedCode="missing_package>=1.0.0",
            explanation="ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄÎ•º requirements.txtÏóê Ï∂îÍ∞Ä",
        )]
        tips = ["pip install Ïã§Ìñâ", "Í∞ÄÏÉÅÌôòÍ≤Ω ÌôïÏù∏", "PYTHONPATH ÌôïÏù∏"]
        
    elif "syntaxerror" in error_lower:
        diagnosis = "Î¨∏Î≤ï Ïò§Î•òÏûÖÎãàÎã§. ÏΩîÎìúÏóê Íµ¨Î¨∏ Ïò§Î•òÍ∞Ä ÏûàÏäµÎãàÎã§."
        root_cause = "Í¥ÑÌò∏, ÏΩúÎ°†, Îì§Ïó¨Ïì∞Í∏∞ Îì± Python Î¨∏Î≤ï Í∑úÏπô ÏúÑÎ∞ò"
        fixes = [BugFix(
            filePath=file_path or "unknown.py",
            lineNumber=1,
            originalCode="def func(",
            fixedCode="def func():",
            explanation="Ìï®Ïàò Ï†ïÏùò Î¨∏Î≤ï ÏàòÏ†ï",
        )]
        tips = ["IDEÏùò Î¶∞ÌÑ∞ ÌôúÏÑ±Ìôî", "ÏΩîÎìú Ìè¨Îß∑ÌÑ∞ ÏÇ¨Ïö©", "Í¥ÑÌò∏ Ïßù ÌôïÏù∏"]
        
    else:
        diagnosis = f"""## ÎîîÎ≤ÑÍ∑∏ Î∂ÑÏÑù (Í∞úÎ∞ú Î™®Îìú)

**ÏóêÎü¨**: {error_message or 'ÏóÜÏùå'}

**ÏÑ§Î™Ö**: {description or 'ÏóÜÏùå'}

{'**Ïä§ÌÉù Ìä∏Î†àÏù¥Ïä§**:' + chr(10) + '```' + chr(10) + stack_trace[:500] + chr(10) + '```' if stack_trace else ''}

Ïù¥ Î∂ÑÏÑùÏùÄ Í∞úÎ∞ú Î™®ÎìúÏóêÏÑú ÏÉùÏÑ±Îêú Mock ÏùëÎãµÏûÖÎãàÎã§.
Ïã§Ï†ú AI Î∂ÑÏÑùÏùÑ ÏúÑÌï¥ vLLM ÏÑúÎ≤ÑÎ•º Ïó∞Í≤∞Ìï¥Ï£ºÏÑ∏Ïöî."""
        root_cause = "Í∞úÎ∞ú Î™®ÎìúÏóêÏÑúÎäî Ï†ïÌôïÌïú ÏõêÏù∏ Î∂ÑÏÑùÏù¥ Ï†úÌïúÎê©ÎãàÎã§."
        fixes = []
        tips = ["vLLM ÏÑúÎ≤Ñ Ïó∞Í≤∞ ÌõÑ Ïû¨ÏãúÎèÑ", "ÏóêÎü¨ Î°úÍ∑∏ ÏûêÏÑ∏Ìûà ÌôïÏù∏", "Í¥ÄÎ†® ÏΩîÎìú Í≤ÄÌÜ†"]
    
    return diagnosis, root_cause, fixes, tips


# ============================================================
# AI Mode Status
# ============================================================

@router.get(
    "/modes",
    summary="ÏÇ¨Ïö© Í∞ÄÎä•Ìïú AI Î™®Îìú Î™©Î°ù",
    description="ÌòÑÏû¨ ÏßÄÏõêÌïòÎäî AI Î™®Îìú Î™©Î°ùÍ≥º ÏÑ§Î™ÖÏùÑ Î∞òÌôòÌï©ÎãàÎã§.",
)
async def get_available_modes():
    """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú AI Î™®Îìú Î™©Î°ù Î∞òÌôò"""
    return {
        "modes": [
            {
                "id": "ask",
                "name": "Ask",
                "description": "ÏΩîÎìúÏóê ÎåÄÌï¥ ÏßàÎ¨∏ÌïòÍ≥† ÎãµÎ≥ÄÏùÑ Î∞õÏäµÎãàÎã§.",
                "icon": "üí¨",
            },
            {
                "id": "agent",
                "name": "Agent",
                "description": "ÏûêÎèôÏúºÎ°ú ÏΩîÎìúÎ•º Î∂ÑÏÑùÌïòÍ≥† Î≥ÄÍ≤Ω ÏÇ¨Ìï≠ÏùÑ Ï†úÏïàÌï©ÎãàÎã§.",
                "icon": "ü§ñ",
            },
            {
                "id": "plan",
                "name": "Plan",
                "description": "Î™©ÌëúÎ•º Î∂ÑÏÑùÌïòÍ≥† Îã®Í≥ÑÎ≥Ñ Ïã§Ìñâ Í≥ÑÌöçÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.",
                "icon": "üìã",
            },
            {
                "id": "debug",
                "name": "Debug",
                "description": "ÏóêÎü¨Î•º Î∂ÑÏÑùÌïòÍ≥† Î≤ÑÍ∑∏ ÏàòÏ†ï Î∞©ÏïàÏùÑ Ï†úÏãúÌï©ÎãàÎã§.",
                "icon": "üêõ",
            },
        ],
        "current": "ask",  # Í∏∞Î≥∏ Î™®Îìú
    }


# ============================================================
# Advanced AI Chat with Context & Image (Cursor-like)
# ============================================================

from fastapi import UploadFile, File, Form
from ..models import (
    ContextType,
    ContextItem,
    AIAdvancedChatRequest,
    AIAdvancedChatResponse,
    ImageUploadResponse,
    ImageAnalysisRequest,
    ImageAnalysisResponse,
    ContextSuggestRequest,
    ContextSuggestion,
    ContextSuggestResponse,
)
import os
import uuid
import base64
from pathlib import Path

# Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• Í≤ΩÎ°ú
IMAGE_UPLOAD_DIR = Path(os.getenv("IMAGE_UPLOAD_DIR", "/tmp/ai_images"))
IMAGE_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# ÌóàÏö©Îêú Ïù¥ÎØ∏ÏßÄ ÌòïÏãù
ALLOWED_IMAGE_TYPES = {"image/jpeg", "image/png", "image/gif", "image/webp"}
MAX_IMAGE_SIZE = 10 * 1024 * 1024  # 10MB


@router.post(
    "/image/upload",
    response_model=ImageUploadResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid image"},
        413: {"model": ErrorResponse, "description": "Image too large"},
    },
    summary="Ïù¥ÎØ∏ÏßÄ ÏóÖÎ°úÎìú",
    description="AI Î∂ÑÏÑùÏùÑ ÏúÑÌïú Ïù¥ÎØ∏ÏßÄÎ•º ÏóÖÎ°úÎìúÌï©ÎãàÎã§.",
)
async def upload_image(
    file: UploadFile = File(..., description="ÏóÖÎ°úÎìúÌï† Ïù¥ÎØ∏ÏßÄ ÌååÏùº"),
):
    """
    Ïù¥ÎØ∏ÏßÄ ÏóÖÎ°úÎìú
    
    - Ïä§ÌÅ¨Î¶∞ÏÉ∑, ÏóêÎü¨ ÌôîÎ©¥, UI ÎîîÏûêÏù∏ Îì±ÏùÑ ÏóÖÎ°úÎìú
    - AIÍ∞Ä Ïù¥ÎØ∏ÏßÄÎ•º Î∂ÑÏÑùÌïòÏó¨ ÏΩîÎìú ÏûëÏÑ±Ïóê ÌôúÏö©
    """
    # MIME ÌÉÄÏûÖ Í≤ÄÏ¶ù
    if file.content_type not in ALLOWED_IMAGE_TYPES:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": f"Invalid image type. Allowed: {', '.join(ALLOWED_IMAGE_TYPES)}", "code": "INVALID_IMAGE_TYPE"},
        )
    
    # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏
    content = await file.read()
    if len(content) > MAX_IMAGE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail={"error": f"Image too large. Max size: {MAX_IMAGE_SIZE // 1024 // 1024}MB", "code": "IMAGE_TOO_LARGE"},
        )
    
    # Í≥†Ïú† ID ÏÉùÏÑ±
    image_id = str(uuid.uuid4())
    ext = file.content_type.split("/")[-1]
    if ext == "jpeg":
        ext = "jpg"
    
    # ÌååÏùº Ï†ÄÏû•
    file_path = IMAGE_UPLOAD_DIR / f"{image_id}.{ext}"
    with open(file_path, "wb") as f:
        f.write(content)
    
    # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï†ïÎ≥¥ (PIL ÏóÜÏù¥ Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©)
    width, height = None, None
    try:
        # PILÏù¥ ÏûàÏúºÎ©¥ ÏÇ¨Ïö©
        from PIL import Image
        with Image.open(file_path) as img:
            width, height = img.size
    except ImportError:
        pass
    
    # ÏÉÅÎåÄ URL ÏÉùÏÑ±
    image_url = f"/api/ai/image/{image_id}.{ext}"
    
    return ImageUploadResponse(
        image_id=image_id,
        image_url=image_url,
        thumbnail_url=image_url,  # Ïç∏ÎÑ§Ïùº ÏÉùÏÑ± ÎØ∏Íµ¨ÌòÑ
        mime_type=file.content_type,
        size=len(content),
        width=width,
        height=height,
    )


@router.get(
    "/image/{image_filename}",
    summary="Ïù¥ÎØ∏ÏßÄ Ï°∞Ìöå",
    description="ÏóÖÎ°úÎìúÎêú Ïù¥ÎØ∏ÏßÄÎ•º Î∞òÌôòÌï©ÎãàÎã§.",
)
async def get_image(image_filename: str):
    """ÏóÖÎ°úÎìúÎêú Ïù¥ÎØ∏ÏßÄ Ï°∞Ìöå"""
    from fastapi.responses import FileResponse
    
    # Í≤ΩÎ°ú ÌÉàÏ∂ú Î∞©ÏßÄ
    if ".." in image_filename or "/" in image_filename:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Invalid filename", "code": "INVALID_PATH"},
        )
    
    file_path = IMAGE_UPLOAD_DIR / image_filename
    if not file_path.exists():
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail={"error": "Image not found", "code": "IMAGE_NOT_FOUND"},
        )
    
    return FileResponse(file_path)


@router.post(
    "/image/analyze",
    response_model=ImageAnalysisResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        404: {"model": ErrorResponse, "description": "Image not found"},
    },
    summary="Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù",
    description="ÏóÖÎ°úÎìúÎêú Ïù¥ÎØ∏ÏßÄÎ•º AIÎ°ú Î∂ÑÏÑùÌï©ÎãàÎã§.",
)
async def analyze_image(request: ImageAnalysisRequest):
    """
    Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù
    
    - Ïä§ÌÅ¨Î¶∞ÏÉ∑ÏóêÏÑú ÏóêÎü¨ Î©îÏãúÏßÄ Ï∂îÏ∂ú
    - UI ÎîîÏûêÏù∏ÏóêÏÑú ÏΩîÎìú ÏÉùÏÑ±
    - Îã§Ïù¥Ïñ¥Í∑∏Îû® Ìï¥ÏÑù
    
    TODO: Vision LLM Ïó∞Îèô (GPT-4V, LLaVA Îì±)
    """
    # Í∞úÎ∞ú Î™®Îìú Mock ÏùëÎãµ
    dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
    
    if dev_mode:
        question = request.question or "Ïù¥ÎØ∏ÏßÄÎ•º Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî."
        return ImageAnalysisResponse(
            description=f"""## Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Í≤∞Í≥º (Í∞úÎ∞ú Î™®Îìú)

**ÏßàÎ¨∏**: {question}

Ïù¥ ÏùëÎãµÏùÄ Í∞úÎ∞ú Î™®ÎìúÏóêÏÑú ÏÉùÏÑ±Îêú Mock ÏùëÎãµÏûÖÎãàÎã§.

Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑùÏùÑ ÏúÑÌï¥ÏÑúÎäî Vision LLM (GPT-4V, LLaVA Îì±)ÏùÑ Ïó∞Í≤∞Ìï¥Ï£ºÏÑ∏Ïöî.

**ÏßÄÏõê ÏòàÏ†ï Í∏∞Îä•**:
- Ïä§ÌÅ¨Î¶∞ÏÉ∑ÏóêÏÑú ÏóêÎü¨ Î©îÏãúÏßÄ Ï∂îÏ∂ú (OCR)
- UI ÎîîÏûêÏù∏ÏóêÏÑú ÏΩîÎìú ÏÉùÏÑ±
- Îã§Ïù¥Ïñ¥Í∑∏Îû®/ÌîåÎ°úÏö∞Ï∞®Ìä∏ Ìï¥ÏÑù
- ÏΩîÎìú Ïä§ÎãàÌé´ Ï∂îÏ∂ú
""",
            extracted_text="[Í∞úÎ∞ú Î™®Îìú] OCR ÌÖçÏä§Ìä∏ Ï∂îÏ∂úÏù¥ ÎπÑÌôúÏÑ±ÌôîÎêòÏñ¥ ÏûàÏäµÎãàÎã§.",
            code_blocks=["# Í∞úÎ∞ú Î™®ÎìúÏóêÏÑúÎäî ÏΩîÎìú Ï∂îÏ∂úÏù¥ ÏßÄÏõêÎêòÏßÄ ÏïäÏäµÎãàÎã§."],
        )
    
    # TODO: Ïã§Ï†ú Vision LLM Ïó∞Îèô
    raise HTTPException(
        status_code=status.HTTP_501_NOT_IMPLEMENTED,
        detail={"error": "Vision LLM not implemented yet", "code": "NOT_IMPLEMENTED"},
    )


@router.post(
    "/context/suggest",
    response_model=ContextSuggestResponse,
    summary="Ïª®ÌÖçÏä§Ìä∏ Ï†úÏïà",
    description="ÏûÖÎ†•Ïóê Îî∞Îùº Í¥ÄÎ†® ÌååÏùº/ÏΩîÎìúÎ•º Ï†úÏïàÌï©ÎãàÎã§.",
)
async def suggest_context(request: ContextSuggestRequest):
    """
    Ïª®ÌÖçÏä§Ìä∏ Ï†úÏïà (@ ÏûÖÎ†• Ïãú)
    
    ÏÇ¨Ïö©ÏûêÍ∞Ä "@"Î•º ÏûÖÎ†•ÌïòÎ©¥ Í¥ÄÎ†® ÌååÏùº, Ìè¥Îçî, Ïã¨Î≥ºÏùÑ Ï†úÏïàÌï©ÎãàÎã§.
    """
    import os
    from pathlib import Path
    
    workspace_root = Path(f"/workspaces/{request.workspace_id}")
    query = request.query.lower()
    suggestions = []
    
    # ÌååÏùº Í≤ÄÏÉâ
    if not workspace_root.exists():
        return ContextSuggestResponse(suggestions=[], total=0)
    
    try:
        for root, dirs, files in os.walk(workspace_root):
            # Ïà®ÍπÄ Ìè¥Îçî Ï†úÏô∏
            dirs[:] = [d for d in dirs if not d.startswith(".")]
            
            rel_root = Path(root).relative_to(workspace_root)
            
            # Ìè¥Îçî Í≤ÄÏÉâ
            if not request.types or ContextType.FOLDER in request.types:
                for d in dirs:
                    if query in d.lower():
                        path = str(rel_root / d)
                        suggestions.append(ContextSuggestion(
                            type=ContextType.FOLDER,
                            path=path,
                            name=d,
                            preview=f"üìÅ {path}",
                            relevance=0.8 if d.lower().startswith(query) else 0.5,
                        ))
            
            # ÌååÏùº Í≤ÄÏÉâ
            if not request.types or ContextType.FILE in request.types:
                for f in files:
                    if query in f.lower():
                        path = str(rel_root / f)
                        suggestions.append(ContextSuggestion(
                            type=ContextType.FILE,
                            path=path,
                            name=f,
                            preview=f"üìÑ {path}",
                            relevance=0.9 if f.lower().startswith(query) else 0.6,
                        ))
            
            # ÎÑàÎ¨¥ ÎßéÏù¥ ÌÉêÏÉâÌïòÏßÄ ÏïäÎèÑÎ°ù Ï†úÌïú
            if len(suggestions) >= request.limit * 2:
                break
    except Exception:
        pass
    
    # Í¥ÄÎ†®ÎèÑÏàú Ï†ïÎ†¨ Î∞è Ï†úÌïú
    suggestions.sort(key=lambda x: x.relevance, reverse=True)
    suggestions = suggestions[:request.limit]
    
    return ContextSuggestResponse(suggestions=suggestions, total=len(suggestions))


@router.post(
    "/advanced/chat",
    response_model=AIAdvancedChatResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="Í≥†Í∏â AI Ï±ÑÌåÖ (Cursor Ïä§ÌÉÄÏùº)",
    description="Ïó¨Îü¨ Ïª®ÌÖçÏä§Ìä∏, Ïù¥ÎØ∏ÏßÄÎ•º Ìè¨Ìï®Ìïú AI Ï±ÑÌåÖÏûÖÎãàÎã§.",
)
async def advanced_chat(request: AIAdvancedChatRequest):
    """
    Í≥†Í∏â AI Ï±ÑÌåÖ
    
    - Ïó¨Îü¨ ÌååÏùº/Ìè¥ÎçîÎ•º Ïª®ÌÖçÏä§Ìä∏Î°ú Ï∂îÍ∞Ä
    - Ïù¥ÎØ∏ÏßÄ Ï≤®Î∂Ä
    - ÌÅ¥Î¶ΩÎ≥¥Îìú ÌÖçÏä§Ìä∏/Ïù¥ÎØ∏ÏßÄ
    - Agent/Plan/Debug/Ask Î™®Îìú ÏßÄÏõê
    """
    import os
    dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
    
    # Ïª®ÌÖçÏä§Ìä∏ Ï°∞Ìï©
    context_parts = []
    
    # ÌòÑÏû¨ Ïó¥Î¶∞ ÌååÏùº
    if request.current_file and request.current_content:
        context_parts.append(f"**ÌòÑÏû¨ ÌååÏùº ({request.current_file})**:\n```\n{request.current_content[:2000]}\n```")
        if request.current_selection:
            lines = request.current_content.split("\n")
            selected = "\n".join(lines[request.current_selection.start_line - 1:request.current_selection.end_line])
            context_parts.append(f"**ÏÑ†ÌÉùÎêú ÏΩîÎìú (L{request.current_selection.start_line}-{request.current_selection.end_line})**:\n```\n{selected}\n```")
    
    # Ï∂îÍ∞Ä Ïª®ÌÖçÏä§Ìä∏
    if request.contexts:
        for ctx in request.contexts:
            if ctx.type == ContextType.FILE and ctx.content:
                context_parts.append(f"**ÌååÏùº ({ctx.path or ctx.name})**:\n```\n{ctx.content[:1500]}\n```")
            elif ctx.type == ContextType.IMAGE:
                if ctx.image_url:
                    context_parts.append(f"**Ïù¥ÎØ∏ÏßÄ**: {ctx.image_url}")
                elif ctx.image_base64:
                    context_parts.append(f"**Ïù¥ÎØ∏ÏßÄ (Base64)**: [Ïù¥ÎØ∏ÏßÄ Ï≤®Î∂ÄÎê®]")
            elif ctx.type == ContextType.CLIPBOARD and ctx.content:
                context_parts.append(f"**ÌÅ¥Î¶ΩÎ≥¥Îìú**:\n```\n{ctx.content[:1000]}\n```")
            elif ctx.type == ContextType.SELECTION and ctx.content:
                context_parts.append(f"**ÏÑ†ÌÉù ÏòÅÏó≠**:\n```\n{ctx.content[:1000]}\n```")
    
    full_context = "\n\n".join(context_parts) if context_parts else ""
    
    # Î™®ÎìúÎ≥Ñ Ï≤òÎ¶¨
    if dev_mode:
        if request.mode == AIMode.PLAN:
            return AIAdvancedChatResponse(
                response=f"""## ÏûëÏóÖ Í≥ÑÌöç (Í∞úÎ∞ú Î™®Îìú)

**Î™©Ìëú**: {request.message}

{f'**Ïª®ÌÖçÏä§Ìä∏**:{chr(10)}{full_context[:500]}...' if full_context else ''}

### Ïã§Ìñâ Í≥ÑÌöç:
1. ÏöîÍµ¨ÏÇ¨Ìï≠ Î∂ÑÏÑù
2. ÏÑ§Í≥Ñ Í≤ÄÌÜ†
3. Íµ¨ÌòÑ
4. ÌÖåÏä§Ìä∏
5. Î∞∞Ìè¨

*Ïù¥ ÏùëÎãµÏùÄ Í∞úÎ∞ú Î™®ÎìúÏóêÏÑú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§. Ïã§Ï†ú AI Î∂ÑÏÑùÏùÑ ÏúÑÌï¥ vLLMÏùÑ Ïó∞Í≤∞ÌïòÏÑ∏Ïöî.*
""",
                mode=AIMode.PLAN,
                tokens_used=0,
                plan_steps=[
                    TaskStep(step_number=1, description="ÏöîÍµ¨ÏÇ¨Ìï≠ Î∂ÑÏÑù", status="pending"),
                    TaskStep(step_number=2, description="ÏÑ§Í≥Ñ Í≤ÄÌÜ†", status="pending"),
                    TaskStep(step_number=3, description="Íµ¨ÌòÑ", status="pending"),
                    TaskStep(step_number=4, description="ÌÖåÏä§Ìä∏", status="pending"),
                    TaskStep(step_number=5, description="Î∞∞Ìè¨", status="pending"),
                ],
            )
        elif request.mode == AIMode.AGENT:
            return AIAdvancedChatResponse(
                response=f"""## ÏΩîÎìú Î≥ÄÍ≤Ω Ï†úÏïà (Í∞úÎ∞ú Î™®Îìú)

**ÏöîÏ≤≠**: {request.message}

{f'**Ï∞∏Ï°∞Ìïú Ïª®ÌÖçÏä§Ìä∏**: {len(request.contexts or [])}Í∞ú' if request.contexts else ''}

### Î≥ÄÍ≤Ω ÏÇ¨Ìï≠:
ÏïÑÎûòÎäî Í∞úÎ∞ú Î™®Îìú Mock ÏùëÎãµÏûÖÎãàÎã§.

*Ïã§Ï†ú ÏΩîÎìú ÏÉùÏÑ±ÏùÑ ÏúÑÌï¥ vLLMÏùÑ Ïó∞Í≤∞ÌïòÏÑ∏Ïöî.*
""",
                mode=AIMode.AGENT,
                tokens_used=0,
                file_changes=[
                    FileChange(
                        file_path=request.current_file or "example.py",
                        action="modify",
                        content="# Agent Î™®Îìú (Í∞úÎ∞ú)\n# Ïã§Ï†ú Î≥ÄÍ≤Ω ÏÇ¨Ìï≠ÏùÄ vLLM Ïó∞Í≤∞ ÌõÑ ÏÉùÏÑ±Îê©ÎãàÎã§.",
                        diff="@@ -1 +1,2 @@\n+# Agent Î™®Îìú ÏòàÏãú",
                    )
                ],
            )
        elif request.mode == AIMode.DEBUG:
            return AIAdvancedChatResponse(
                response=f"""## ÎîîÎ≤ÑÍ∑∏ Î∂ÑÏÑù (Í∞úÎ∞ú Î™®Îìú)

**Î¨∏Ï†ú**: {request.message}

### Î∂ÑÏÑù:
Í∞úÎ∞ú Î™®ÎìúÏóêÏÑúÎäî Ï†úÌïúÏ†ÅÏù∏ Î∂ÑÏÑùÎßå Í∞ÄÎä•Ìï©ÎãàÎã§.

### Ï†úÏïà:
1. ÏóêÎü¨ Î°úÍ∑∏ ÌôïÏù∏
2. Í¥ÄÎ†® ÏΩîÎìú Í≤ÄÌÜ†
3. vLLM Ïó∞Í≤∞ ÌõÑ ÏÉÅÏÑ∏ Î∂ÑÏÑù

*Ïã§Ï†ú ÎîîÎ≤ÑÍπÖÏùÑ ÏúÑÌï¥ vLLMÏùÑ Ïó∞Í≤∞ÌïòÏÑ∏Ïöî.*
""",
                mode=AIMode.DEBUG,
                tokens_used=0,
                bug_fixes=[
                    BugFix(
                        filePath=request.current_file or "unknown.py",
                        lineNumber=None,
                        originalCode="",
                        fixedCode="# ÏàòÏ†ï ÏΩîÎìúÎäî vLLM Ïó∞Í≤∞ ÌõÑ ÏÉùÏÑ±Îê©ÎãàÎã§",
                        explanation="Í∞úÎ∞ú Î™®ÎìúÏóêÏÑúÎäî Ïã§Ï†ú Î≤ÑÍ∑∏ ÏàòÏ†ïÏù¥ ÏÉùÏÑ±ÎêòÏßÄ ÏïäÏäµÎãàÎã§.",
                    )
                ],
            )
        else:  # ASK Î™®Îìú
            return AIAdvancedChatResponse(
                response=f"""## ÎãµÎ≥Ä (Í∞úÎ∞ú Î™®Îìú)

**ÏßàÎ¨∏**: {request.message}

{f'**Ïª®ÌÖçÏä§Ìä∏**: {len(request.contexts or [])}Í∞ú Ìï≠Î™© Ï∞∏Ï°∞' if request.contexts else ''}

### ÏùëÎãµ:
Í∞úÎ∞ú Î™®ÎìúÏóêÏÑú ÏÉùÏÑ±Îêú Mock ÏùëÎãµÏûÖÎãàÎã§.

{f'ÌòÑÏû¨ ÌååÏùº: `{request.current_file}`' if request.current_file else ''}

Ïã§Ï†ú AI ÎãµÎ≥ÄÏùÑ ÏúÑÌï¥ vLLM ÏÑúÎ≤ÑÎ•º Ïó∞Í≤∞Ìï¥Ï£ºÏÑ∏Ïöî.

*VLLM_BASE_URL ÌôòÍ≤ΩÎ≥ÄÏàòÎ•º ÏÑ§Ï†ïÌïòÍ≥† DEV_MODE=falseÎ°ú Î≥ÄÍ≤ΩÌïòÏÑ∏Ïöî.*
""",
                mode=AIMode.ASK,
                tokens_used=0,
            )
    
    # Ïã§Ï†ú LLM Ìò∏Ï∂ú (ÎπÑÍ∞úÎ∞ú Î™®Îìú)
    try:
        llm_client = get_llm_client()
        
        # ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ†ÌÉù
        mode_prompts = {
            AIMode.ASK: "You are a helpful coding assistant. Answer questions about code clearly and concisely.",
            AIMode.AGENT: "You are a coding agent. Analyze the code and suggest specific changes. Provide full code for modifications.",
            AIMode.PLAN: "You are a project planner. Break down the task into clear, actionable steps.",
            AIMode.DEBUG: "You are a debugging expert. Analyze errors and suggest specific fixes.",
        }
        
        system_prompt = mode_prompts.get(request.mode, mode_prompts[AIMode.ASK])
        
        # Î©îÏãúÏßÄ Íµ¨ÏÑ±
        messages = [{"role": "system", "content": system_prompt}]
        
        # ÌûàÏä§ÌÜ†Î¶¨ Ï∂îÍ∞Ä
        if request.history:
            for msg in request.history[-10:]:  # ÏµúÍ∑º 10Í∞ú
                messages.append({"role": msg.role, "content": msg.content})
        
        # Ïª®ÌÖçÏä§Ìä∏ + ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ
        user_message = request.message
        if full_context:
            user_message = f"{full_context}\n\n---\n\n**ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠**: {request.message}"
        
        messages.append({"role": "user", "content": user_message})
        
        # LLM Ìò∏Ï∂ú
        llm_response = await llm_client.chat(messages=messages)
        
        # OpenAI API ÏùëÎãµ ÌòïÏãùÏóêÏÑú content Ï∂îÏ∂ú
        response_content = ""
        tokens_used = 0
        
        if "choices" in llm_response and len(llm_response["choices"]) > 0:
            response_content = llm_response["choices"][0].get("message", {}).get("content", "")
        
        if "usage" in llm_response:
            tokens_used = llm_response["usage"].get("total_tokens", 0)
        
        return AIAdvancedChatResponse(
            response=response_content,
            mode=request.mode,
            tokens_used=tokens_used,
        )
        
    except (LLMTimeoutError, LLMError) as e:
        return AIAdvancedChatResponse(
            response=f"‚ö†Ô∏è LLM Ïó∞Í≤∞ Ïã§Ìå®: {str(e)}\n\nvLLM ÏÑúÎ≤Ñ ÏÉÅÌÉúÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.",
            mode=request.mode,
            tokens_used=0,
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": str(e), "code": "INTERNAL_ERROR"},
        )
