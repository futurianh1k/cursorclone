"""
AI ë¼ìš°í„°
- POST /api/ai/explain   - ì½”ë“œ ì„¤ëª…
- POST /api/ai/chat      - ëŒ€í™”í˜• ì±„íŒ…
- POST /api/ai/rewrite   - ì½”ë“œ ìˆ˜ì •
- POST /api/ai/plan      - ì‘ì—… ê³„íš ìˆ˜ë¦½
- POST /api/ai/agent     - ìë™ ì½”ë“œ ì‘ì„±/ìˆ˜ì •
- POST /api/ai/debug     - ë²„ê·¸ ë¶„ì„/ìˆ˜ì •
"""

from fastapi import APIRouter, HTTPException, status
from ..models import (
    AIExplainRequest,
    AIExplainResponse,
    AIChatRequest,
    AIChatResponse,
    AIRewriteRequest,
    AIRewriteResponse,
    # AI Modes
    AIMode,
    AIPlanRequest,
    AIPlanResponse,
    TaskStep,
    AIAgentRequest,
    AIAgentResponse,
    FileChange,
    AIDebugRequest,
    AIDebugResponse,
    BugFix,
    ErrorResponse,
)
from ..context_builder import (
    DefaultContextBuilder,
    ContextBuildRequest,
    ContextSource,
    ContextSourceType,
    ActionType,
    SecurityError,
)
from ..llm import (
    get_llm_client,
    LLMError,
    LLMTimeoutError,
)

router = APIRouter(prefix="/api/ai", tags=["ai"])

# Context Builder ì¸ìŠ¤í„´ìŠ¤ (ì „ì—­ ë˜ëŠ” ì˜ì¡´ì„± ì£¼ì…)
# TODO: ì‹¤ì œë¡œëŠ” ì˜ì¡´ì„± ì£¼ì… ë˜ëŠ” ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸°
_context_builder_cache = {}


def _get_context_builder(workspace_id: str) -> DefaultContextBuilder:
    """
    ì›Œí¬ìŠ¤í˜ì´ìŠ¤ë³„ Context Builder ê°€ì ¸ì˜¤ê¸°
    
    TODO: ì‹¤ì œ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë£¨íŠ¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    """
    if workspace_id not in _context_builder_cache:
        # TODO: ì‹¤ì œ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë£¨íŠ¸ ê²½ë¡œ
        workspace_root = f"/workspaces/{workspace_id}"
        _context_builder_cache[workspace_id] = DefaultContextBuilder(
            workspace_root=workspace_root,
        )
    return _context_builder_cache[workspace_id]


def _validate_workspace_access(ws_id: str) -> bool:
    """ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì ‘ê·¼ ê¶Œí•œ ê²€ì¦"""
    # TODO: ì‹¤ì œ ê¶Œí•œ ê²€ì¦ ë¡œì§ êµ¬í˜„
    return True


def _validate_path(path: str) -> bool:
    """ê²½ë¡œ ê²€ì¦ (íƒˆì¶œ ë°©ì§€)"""
    if ".." in path:
        return False
    if path.startswith("/"):
        return False
    return True


async def _read_file_content(workspace_id: str, file_path: str) -> str:
    """
    íŒŒì¼ ë‚´ìš© ì½ê¸° (files ë¼ìš°í„°ì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©)
    """
    import os
    from ..utils.filesystem import get_workspace_root, validate_path, read_file_content, workspace_exists
    
    workspace_root = get_workspace_root(workspace_id)
    
    # ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not workspace_exists(workspace_root):
        raise ValueError(f"Workspace not found: {workspace_id}")
    
    # ê²½ë¡œ ê²€ì¦ ë° íŒŒì¼ ì½ê¸°
    try:
        full_path = validate_path(file_path, workspace_root)
        content, _ = read_file_content(full_path)
        return content
    except (ValueError, FileNotFoundError) as e:
        raise ValueError(f"Failed to read file: {e}")


@router.post(
    "/explain",
    response_model=AIExplainResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        401: {"model": ErrorResponse, "description": "Unauthorized"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        404: {"model": ErrorResponse, "description": "File not found"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ì½”ë“œ ì„¤ëª…",
    description="ì„ íƒëœ ì½”ë“œì— ëŒ€í•œ AI ì„¤ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
)
async def explain_code(request: AIExplainRequest):
    """
    ì„ íƒëœ ì½”ë“œì— ëŒ€í•œ AI ì„¤ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    TODO: ì‹¤ì œ AI ì„¤ëª… êµ¬í˜„
    - Context Builderë¥¼ í†µí•´ ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
    - vLLMìœ¼ë¡œ ìš”ì²­ ì „ì†¡
    - ì‘ë‹µ ì²˜ë¦¬ ë° ë°˜í™˜
    
    íë¦„: API â†’ Context Builder â†’ vLLM â†’ ì‘ë‹µ
    
    âš ï¸ ì£¼ì˜ (AGENTS.md ê·œì¹™)
    - APIëŠ” LLMì„ ì§ì ‘ í˜¸ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
    - ë°˜ë“œì‹œ Context Builderë¥¼ ê²½ìœ í•´ì•¼ í•©ë‹ˆë‹¤
    """
    # ê²½ë¡œ ê²€ì¦
    if not _validate_path(request.file_path):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Invalid path", "code": "AI_INVALID_PATH"},
        )
    
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    try:
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        file_content = await _read_file_content(request.workspace_id, request.file_path)
        
        # Context Builder ì¤€ë¹„
        context_builder = _get_context_builder(request.workspace_id)
        
        # ì»¨í…ìŠ¤íŠ¸ ì†ŒìŠ¤ êµ¬ì„±
        sources = [
            ContextSource(
                type=ContextSourceType.FILE if not request.selection else ContextSourceType.SELECTION,
                path=request.file_path,
                content=file_content,
                range=request.selection,
            )
        ]
        
        # Context Builder í˜¸ì¶œ
        context_request = ContextBuildRequest(
            workspace_id=request.workspace_id,
            action=ActionType.EXPLAIN,
            instruction=f"ë‹¤ìŒ ì½”ë“œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”: {request.file_path}",
            sources=sources,
        )
        
        context_response = await context_builder.build(context_request)
        
        # vLLM í˜¸ì¶œ (ê°œë°œ ëª¨ë“œì—ì„œëŠ” Mock ì‘ë‹µ)
        import os
        dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
        
        try:
            if dev_mode:
                # ê°œë°œ ëª¨ë“œ: Mock ì‘ë‹µ ìƒì„±
                # ì„ íƒëœ ì½”ë“œ ì¶”ì¶œ
                if request.selection:
                    lines = file_content.split("\n")
                    start = request.selection.start_line - 1
                    end = request.selection.end_line
                    selected_code = "\n".join(lines[start:end])
                else:
                    selected_code = file_content[:500] + ("..." if len(file_content) > 500 else "")
                
                explanation = f"""## ì½”ë“œ ë¶„ì„ (ê°œë°œ ëª¨ë“œ)

**íŒŒì¼**: `{request.file_path}`

### ì½”ë“œ ë‚´ìš©
```
{selected_code}
```

### ì„¤ëª…
ì´ ì½”ë“œëŠ” `{request.file_path}` íŒŒì¼ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.

> âš ï¸ **ê°œë°œ ëª¨ë“œ**: ì‹¤ì œ LLM ì„œë¹„ìŠ¤ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
> vLLM ì„œë²„ë¥¼ ì‹œì‘í•˜ë©´ ì‹¤ì œ AI ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**vLLM ì„¤ì • ë°©ë²•**:
```bash
# docker-compose.ymlì— vLLM ì„œë¹„ìŠ¤ ì¶”ê°€ ë˜ëŠ”
# VLLM_BASE_URL í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export VLLM_BASE_URL=http://your-vllm-server:8000/v1
```
"""
                return AIExplainResponse(
                    explanation=explanation,
                    tokensUsed=0,
                )
            
            # í”„ë¡œë•ì…˜ ëª¨ë“œ: ì‹¤ì œ LLM í˜¸ì¶œ
            llm_client = get_llm_client()
            
            # ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
            messages = [
                {"role": msg.role, "content": msg.content}
                for msg in context_response.messages
            ]
            
            llm_response = await llm_client.chat(
                messages=messages,
                max_tokens=request.max_tokens if hasattr(request, "max_tokens") else None,
            )
            
            # ì‘ë‹µ ì¶”ì¶œ
            explanation = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            if not explanation:
                explanation = "[LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤]"
            
            return AIExplainResponse(
                explanation=explanation,
                tokensUsed=tokens_used,
            )
            
        except LLMTimeoutError as e:
            raise HTTPException(
                status_code=status.HTTP_504_GATEWAY_TIMEOUT,
                detail={"error": "LLM timeout", "code": "LLM_TIMEOUT", "detail": str(e)},
            )
        except LLMError as e:
            # LLM ì—ëŸ¬ ì‹œ ê°œë°œ ëª¨ë“œ ì‘ë‹µ ì œê³µ
            explanation = f"""## LLM ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨

**ì—ëŸ¬**: {str(e)}

vLLM ì„œë²„ê°€ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•Šê±°ë‚˜ ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**í•´ê²° ë°©ë²•**:
1. vLLM ì„œë²„ ìƒíƒœ í™•ì¸
2. `VLLM_BASE_URL` í™˜ê²½ë³€ìˆ˜ í™•ì¸
3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
"""
            return AIExplainResponse(
                explanation=explanation,
                tokensUsed=0,
            )
        
    except SecurityError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Security validation failed", "code": "SECURITY_ERROR", "detail": str(e)},
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR"},
        )


@router.post(
    "/chat",
    response_model=AIChatResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        401: {"model": ErrorResponse, "description": "Unauthorized"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="AI ì±„íŒ…",
    description="ì½”ë“œì— ëŒ€í•œ ì§ˆë¬¸, ìˆ˜ì • ìš”ì²­, ì£¼ì„ ì¶”ê°€ ë“± ììœ ë¡œìš´ ëŒ€í™”í˜• AI ì¸í„°í˜ì´ìŠ¤.",
)
async def chat_with_ai(request: AIChatRequest):
    """
    ëŒ€í™”í˜• AI ì¸í„°í˜ì´ìŠ¤.
    
    - ì‚¬ìš©ì ì§ˆë¬¸ + íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ LLMì— ì „ë‹¬
    - "ì£¼ì„ ë‹¬ì•„ì¤˜", "ì´ ì½”ë“œ ë­ì•¼?", "ë²„ê·¸ ì°¾ì•„ì¤˜" ë“± ììœ ë¡œìš´ ìš”ì²­ ì²˜ë¦¬
    - íŒŒì¼ì´ ì—†ì–´ë„ ì¼ë°˜ ì§ˆë¬¸ ê°€ëŠ¥
    
    íë¦„: API â†’ Context Builder â†’ vLLM â†’ ì‘ë‹µ
    """
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    # íŒŒì¼ ê²½ë¡œ ê²€ì¦
    if request.file_path and not _validate_path(request.file_path):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Invalid path", "code": "AI_INVALID_PATH"},
        )
    
    try:
        import os
        dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
        
        # íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        file_content = None
        if request.file_path:
            try:
                if request.file_content:
                    file_content = request.file_content
                else:
                    file_content = await _read_file_content(request.workspace_id, request.file_path)
            except Exception as e:
                # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨í•´ë„ ì±„íŒ…ì€ ê³„ì†
                file_content = f"[íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}]"
        
        # ì„ íƒëœ ì½”ë“œ ì¶”ì¶œ
        selected_code = None
        if file_content and request.selection:
            lines = file_content.split("\n")
            start = request.selection.start_line - 1
            end = request.selection.end_line
            selected_code = "\n".join(lines[start:end])
        
        if dev_mode:
            # ê°œë°œ ëª¨ë“œ: Mock ì‘ë‹µ ìƒì„±
            response_text = _generate_mock_chat_response(
                message=request.message,
                file_path=request.file_path,
                file_content=file_content,
                selected_code=selected_code,
            )
            
            return AIChatResponse(
                response=response_text,
                tokensUsed=0,
                suggestedAction=_detect_action(request.message),
            )
        
        # í”„ë¡œë•ì…˜ ëª¨ë“œ: ì‹¤ì œ LLM í˜¸ì¶œ
        try:
            llm_client = get_llm_client()
            
            # íŒŒì¼ì´ ìˆìœ¼ë©´ Context Builder ì‚¬ìš©, ì—†ìœ¼ë©´ ì§ì ‘ í˜¸ì¶œ
            if file_content:
                context_builder = _get_context_builder(request.workspace_id)
                
                sources = [
                    ContextSource(
                        type=ContextSourceType.SELECTION if selected_code else ContextSourceType.FILE,
                        path=request.file_path,
                        content=file_content,
                        range=request.selection,
                    )
                ]
                
                context_request = ContextBuildRequest(
                    workspace_id=request.workspace_id,
                    action=ActionType.EXPLAIN,
                    instruction=request.message,
                    sources=sources,
                )
                
                context_response = await context_builder.build(context_request)
                messages = [
                    {"role": msg.role, "content": msg.content}
                    for msg in context_response.messages
                ]
            else:
                # íŒŒì¼ ì—†ì´ ì¼ë°˜ ì±„íŒ…
                messages = [
                    {"role": "system", "content": "You are a helpful coding assistant. Answer questions about programming, software development, and technology. Respond in the same language as the user's question."},
                    {"role": "user", "content": request.message},
                ]
            
            llm_response = await llm_client.chat(messages=messages)
            
            response_text = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            return AIChatResponse(
                response=response_text or "[ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤]",
                tokensUsed=tokens_used,
                suggestedAction=_detect_action(request.message),
            )
            
        except (LLMTimeoutError, LLMError) as e:
            # LLM ì—ëŸ¬ ì‹œ ì¹œí™”ì ì¸ ì‘ë‹µ
            return AIChatResponse(
                response=f"âš ï¸ LLM ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n**ì—ëŸ¬**: {str(e)}\n\n`VLLM_BASE_URL` í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
                tokensUsed=0,
            )
            
    except SecurityError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Security validation failed", "code": "SECURITY_ERROR", "detail": str(e)},
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR", "detail": str(e)},
        )


def _detect_action(message: str) -> str | None:
    """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ì œì•ˆ ì•¡ì…˜ ê°ì§€"""
    message_lower = message.lower()
    
    rewrite_keywords = ["ìˆ˜ì •", "ë³€ê²½", "ë°”ê¿”", "ì¶”ê°€", "ì‚­ì œ", "ì£¼ì„", "ë¦¬íŒ©í„°", "fix", "change", "add", "remove", "comment"]
    explain_keywords = ["ì„¤ëª…", "ë­ì•¼", "ë­”ê°€ìš”", "ì•Œë ¤ì¤˜", "ì–´ë–»ê²Œ", "explain", "what", "how", "why"]
    
    if any(kw in message_lower for kw in rewrite_keywords):
        return "rewrite"
    if any(kw in message_lower for kw in explain_keywords):
        return "explain"
    
    return None


def _generate_mock_chat_response(
    message: str,
    file_path: str | None,
    file_content: str | None,
    selected_code: str | None,
) -> str:
    """ê°œë°œ ëª¨ë“œìš© Mock ì‘ë‹µ ìƒì„±"""
    # ì‚¬ìš©ì ìš”ì²­ ë¶„ì„
    action = _detect_action(message)
    
    if not file_path:
        return f"""ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì½”ë“œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì§ˆë¬¸**: {message}

í˜„ì¬ íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—´ê³  ì½”ë“œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì‹œë©´ ë” ì •í™•í•œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---
> âš ï¸ **ê°œë°œ ëª¨ë“œ**: vLLM ì„œë²„ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
"""
    
    code_preview = selected_code or (file_content[:300] + "..." if file_content and len(file_content) > 300 else file_content)
    
    if action == "rewrite" and "ì£¼ì„" in message:
        # ì£¼ì„ ì¶”ê°€ ìš”ì²­ì— ëŒ€í•œ Mock ì‘ë‹µ
        return f"""## ì£¼ì„ ì¶”ê°€ ì œì•ˆ

**íŒŒì¼**: `{file_path}`

**ìš”ì²­**: {message}

### ì›ë³¸ ì½”ë“œ
```
{code_preview}
```

### ì œì•ˆëœ ë³€ê²½ì‚¬í•­

ì½”ë“œì— ì£¼ì„ì„ ì¶”ê°€í•˜ë ¤ë©´ **Rewrite ëª¨ë“œ**ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:
1. ì£¼ì„ì„ ë‹¬ ì½”ë“œ ì˜ì—­ì„ ì„ íƒ
2. "Rewrite" ëª¨ë“œë¡œ ì „í™˜
3. "ì£¼ì„ ë‹¬ì•„ì¤˜"ë¼ê³  ì…ë ¥

---
> âš ï¸ **ê°œë°œ ëª¨ë“œ**: ì‹¤ì œ ì½”ë“œ ìˆ˜ì •ì€ vLLM ì—°ê²° í›„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
> `VLLM_BASE_URL` í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
"""
    
    return f"""## AI ì‘ë‹µ

**íŒŒì¼**: `{file_path}`

**ì§ˆë¬¸**: {message}

### ì½”ë“œ ë‚´ìš©
```
{code_preview}
```

### ë¶„ì„

ì´ ì½”ë“œëŠ” `{file_path}` íŒŒì¼ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.

{f"ì„ íƒëœ ì˜ì—­ ({len(selected_code.split(chr(10)))}ì¤„)ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤." if selected_code else "ì „ì²´ íŒŒì¼ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤."}

---
> âš ï¸ **ê°œë°œ ëª¨ë“œ**: ì‹¤ì œ AI ë¶„ì„ì€ vLLM ì„œë²„ ì—°ê²° í›„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""


@router.post(
    "/rewrite",
    response_model=AIRewriteResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        401: {"model": ErrorResponse, "description": "Unauthorized"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        404: {"model": ErrorResponse, "description": "File not found"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ì½”ë“œ ë¦¬ë¼ì´íŠ¸",
    description="ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ê³  unified diffë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
)
async def rewrite_code(request: AIRewriteRequest):
    """
    ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ê³  unified diffë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    âš ï¸ ì¤‘ìš”: ì´ APIëŠ” diffë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì‹¤ì œ ì ìš©ì€ /patch/validate â†’ /patch/applyë¥¼ í†µí•´ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
    
    TODO: ì‹¤ì œ AI ë¦¬ë¼ì´íŠ¸ êµ¬í˜„
    - Context Builderë¥¼ í†µí•´ ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
    - vLLMìœ¼ë¡œ ìš”ì²­ ì „ì†¡ (rewrite í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©)
    - unified diff í˜•ì‹ ì‘ë‹µ íŒŒì‹±
    
    íë¦„: API â†’ Context Builder â†’ vLLM â†’ diff ë°˜í™˜
          â†’ (í´ë¼ì´ì–¸íŠ¸) â†’ /patch/validate â†’ /patch/apply
    
    âš ï¸ ì£¼ì˜ (AGENTS.md ê·œì¹™)
    - APIëŠ” LLMì„ ì§ì ‘ í˜¸ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
    - ì½”ë“œ ë³€ê²½ì€ ë°˜ë“œì‹œ Patch ê²½ë¡œë¡œ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤
    """
    target_file = request.target.get("file", "")
    
    # ê²½ë¡œ ê²€ì¦
    if not _validate_path(target_file):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Invalid path", "code": "AI_INVALID_PATH"},
        )
    
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    try:
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        file_content = await _read_file_content(request.workspace_id, target_file)
        
        # ì„ íƒ ë²”ìœ„ ì¶”ì¶œ
        selection_range = None
        if "selection" in request.target:
            from ..context_builder import SelectionRange
            sel = request.target["selection"]
            selection_range = SelectionRange(
                start_line=sel.get("startLine", sel.get("start_line", 1)),
                end_line=sel.get("endLine", sel.get("end_line", 1)),
            )
        
        # Context Builder ì¤€ë¹„
        context_builder = _get_context_builder(request.workspace_id)
        
        # ì»¨í…ìŠ¤íŠ¸ ì†ŒìŠ¤ êµ¬ì„±
        sources = [
            ContextSource(
                type=ContextSourceType.SELECTION if selection_range else ContextSourceType.FILE,
                path=target_file,
                content=file_content,
                range=selection_range,
            )
        ]
        
        # Context Builder í˜¸ì¶œ
        context_request = ContextBuildRequest(
            workspace_id=request.workspace_id,
            action=ActionType.REWRITE,
            instruction=request.instruction,
            sources=sources,
        )
        
        context_response = await context_builder.build(context_request)
        
        # vLLM í˜¸ì¶œ (rewrite í…œí”Œë¦¿ ì‚¬ìš©)
        try:
            llm_client = get_llm_client()
            
            # ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
            messages = [
                {"role": msg.role, "content": msg.content}
                for msg in context_response.messages
            ]
            
            llm_response = await llm_client.chat(
                messages=messages,
                max_tokens=request.max_tokens if hasattr(request, "max_tokens") else None,
            )
            
            # ì‘ë‹µ ì¶”ì¶œ (unified diff í˜•ì‹)
            diff = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            if not diff:
                # ë¹ˆ diffì¸ ê²½ìš° ê¸°ë³¸ í˜•ì‹ ë°˜í™˜
                diff = f"""--- a/{target_file}
+++ b/{target_file}
@@ -1,0 +1,0 @@
"""
            
            return AIRewriteResponse(
                diff=diff,
                tokensUsed=tokens_used,
            )
            
        except LLMTimeoutError as e:
            raise HTTPException(
                status_code=status.HTTP_504_GATEWAY_TIMEOUT,
                detail={"error": "LLM timeout", "code": "LLM_TIMEOUT", "detail": str(e)},
            )
        except LLMError as e:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail={"error": "LLM service error", "code": "LLM_ERROR", "detail": str(e)},
            )
        
    except SecurityError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "Security validation failed", "code": "SECURITY_ERROR", "detail": str(e)},
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR"},
        )


# ============================================================
# AI Plan Mode - ì‘ì—… ê³„íš ìˆ˜ë¦½
# ============================================================

@router.post(
    "/plan",
    response_model=AIPlanResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ì‘ì—… ê³„íš ìˆ˜ë¦½ (Plan ëª¨ë“œ)",
    description="ëª©í‘œë¥¼ ë¶„ì„í•˜ê³  ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤.",
)
async def create_plan(request: AIPlanRequest):
    """
    ì‚¬ìš©ìê°€ ì œì‹œí•œ ëª©í‘œë¥¼ ë¶„ì„í•˜ê³ , ì´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ 
    ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    ì˜ˆì‹œ:
    - "ë¡œê·¸ì¸ ê¸°ëŠ¥ ì¶”ê°€"
    - "í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±"
    - "ì½”ë“œ ë¦¬íŒ©í† ë§"
    """
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    try:
        import os
        dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
        
        # ê´€ë ¨ íŒŒì¼ ë‚´ìš© ìˆ˜ì§‘
        file_contents = {}
        if request.file_paths:
            for fp in request.file_paths[:5]:  # ìµœëŒ€ 5ê°œ íŒŒì¼
                if _validate_path(fp):
                    try:
                        content = await _read_file_content(request.workspace_id, fp)
                        file_contents[fp] = content
                    except:
                        pass
        
        if dev_mode:
            # ê°œë°œ ëª¨ë“œ: Mock ê³„íš ìƒì„±
            steps = _generate_mock_plan_steps(request.goal, file_contents)
            return AIPlanResponse(
                summary=f"'{request.goal}'ë¥¼ ìœ„í•œ ì‹¤í–‰ ê³„íšì…ë‹ˆë‹¤. (ê°œë°œ ëª¨ë“œ)",
                steps=steps,
                estimatedChanges=len(steps),
                tokensUsed=0,
            )
        
        # í”„ë¡œë•ì…˜ ëª¨ë“œ: ì‹¤ì œ LLM í˜¸ì¶œ
        try:
            llm_client = get_llm_client()
            
            # Plan í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            files_context = ""
            if file_contents:
                files_context = "\n\nê´€ë ¨ íŒŒì¼:\n" + "\n".join([
                    f"### {fp}\n```\n{content[:500]}...\n```" 
                    for fp, content in file_contents.items()
                ])
            
            messages = [
                {"role": "system", "content": """You are a planning assistant. Given a goal, create a detailed step-by-step plan.

Output format (JSON):
{
  "summary": "Brief summary of the plan",
  "steps": [
    {"stepNumber": 1, "description": "Step description", "filePath": "optional/file/path.py"},
    ...
  ]
}

Keep the plan focused and actionable. Each step should be clear and specific."""},
                {"role": "user", "content": f"Goal: {request.goal}\n\nContext: {request.context or 'None'}{files_context}"}
            ]
            
            llm_response = await llm_client.chat(messages=messages)
            response_text = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                import json
                # JSON ë¸”ë¡ ì¶”ì¶œ
                if "```json" in response_text:
                    json_str = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    json_str = response_text.split("```")[1].split("```")[0]
                else:
                    json_str = response_text
                    
                plan_data = json.loads(json_str.strip())
                steps = [
                    TaskStep(
                        stepNumber=s.get("stepNumber", i+1),
                        description=s.get("description", ""),
                        filePath=s.get("filePath"),
                    )
                    for i, s in enumerate(plan_data.get("steps", []))
                ]
                summary = plan_data.get("summary", f"Plan for: {request.goal}")
            except:
                # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì‘ë‹µ
                steps = [TaskStep(stepNumber=1, description=response_text[:500])]
                summary = f"Plan for: {request.goal}"
            
            return AIPlanResponse(
                summary=summary,
                steps=steps,
                estimatedChanges=len(steps),
                tokensUsed=tokens_used,
            )
            
        except (LLMTimeoutError, LLMError) as e:
            # LLM ì˜¤ë¥˜ì‹œ ê¸°ë³¸ ê³„íš ë°˜í™˜
            return AIPlanResponse(
                summary=f"âš ï¸ LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}",
                steps=[TaskStep(stepNumber=1, description="LLM ì„œë²„ ì—°ê²° í™•ì¸ í•„ìš”")],
                estimatedChanges=0,
                tokensUsed=0,
            )
            
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR", "detail": str(e)},
        )


def _generate_mock_plan_steps(goal: str, file_contents: dict) -> list:
    """ê°œë°œ ëª¨ë“œìš© Mock ê³„íš ë‹¨ê³„ ìƒì„±"""
    goal_lower = goal.lower()
    
    if "ë¡œê·¸ì¸" in goal_lower or "auth" in goal_lower:
        return [
            TaskStep(stepNumber=1, description="ì‚¬ìš©ì ëª¨ë¸ ë° DB ìŠ¤í‚¤ë§ˆ ì •ì˜", filePath="src/models/user.py"),
            TaskStep(stepNumber=2, description="ë¹„ë°€ë²ˆí˜¸ í•´ì‹± ìœ í‹¸ë¦¬í‹° êµ¬í˜„", filePath="src/utils/auth.py"),
            TaskStep(stepNumber=3, description="ë¡œê·¸ì¸ API ì—”ë“œí¬ì¸íŠ¸ ìƒì„±", filePath="src/routers/auth.py"),
            TaskStep(stepNumber=4, description="JWT í† í° ë°œê¸‰ ë¡œì§ êµ¬í˜„", filePath="src/services/jwt.py"),
            TaskStep(stepNumber=5, description="ë¡œê·¸ì¸ í…ŒìŠ¤íŠ¸ ì‘ì„±", filePath="tests/test_auth.py"),
        ]
    elif "í…ŒìŠ¤íŠ¸" in goal_lower or "test" in goal_lower:
        return [
            TaskStep(stepNumber=1, description="í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • (pytest)", filePath="pytest.ini"),
            TaskStep(stepNumber=2, description="ìœ ë‹› í…ŒìŠ¤íŠ¸ ì‘ì„±", filePath="tests/test_unit.py"),
            TaskStep(stepNumber=3, description="í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±", filePath="tests/test_integration.py"),
            TaskStep(stepNumber=4, description="í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ í™•ì¸"),
        ]
    elif "ë¦¬íŒ©í† ë§" in goal_lower or "refactor" in goal_lower:
        return [
            TaskStep(stepNumber=1, description="ì¤‘ë³µ ì½”ë“œ ì‹ë³„ ë° ë¶„ì„"),
            TaskStep(stepNumber=2, description="ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì¶”ì¶œ", filePath="src/utils/common.py"),
            TaskStep(stepNumber=3, description="ê¸°ì¡´ ì½”ë“œì—ì„œ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •"),
            TaskStep(stepNumber=4, description="í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° í™•ì¸"),
        ]
    else:
        # ì¼ë°˜ì ì¸ ê³„íš
        return [
            TaskStep(stepNumber=1, description=f"'{goal}' ìš”êµ¬ì‚¬í•­ ë¶„ì„"),
            TaskStep(stepNumber=2, description="í•„ìš”í•œ íŒŒì¼/ëª¨ë“ˆ ì‹ë³„"),
            TaskStep(stepNumber=3, description="ì½”ë“œ êµ¬í˜„"),
            TaskStep(stepNumber=4, description="í…ŒìŠ¤íŠ¸ ì‘ì„± ë° ì‹¤í–‰"),
            TaskStep(stepNumber=5, description="ë¬¸ì„œí™”"),
        ]


# ============================================================
# AI Agent Mode - ìë™ ì½”ë“œ ì‘ì„±/ìˆ˜ì •
# ============================================================

@router.post(
    "/agent",
    response_model=AIAgentResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ìë™ ì½”ë“œ ì‘ì„±/ìˆ˜ì • (Agent ëª¨ë“œ)",
    description="ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ë³€ê²½ ì‚¬í•­ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
)
async def run_agent(request: AIAgentRequest):
    """
    ì—ì´ì „íŠ¸ ëª¨ë“œ: ì‚¬ìš©ì ì§€ì‹œì— ë”°ë¼ ìë™ìœ¼ë¡œ ì½”ë“œë¥¼ ë¶„ì„í•˜ê³ ,
    í•„ìš”í•œ íŒŒì¼ì„ ìˆ˜ì •/ìƒì„±/ì‚­ì œí•˜ëŠ” ë³€ê²½ ì‚¬í•­ì„ ì œì•ˆí•©ë‹ˆë‹¤.
    
    auto_apply=Trueì¸ ê²½ìš° ë³€ê²½ ì‚¬í•­ì„ ì§ì ‘ ì ìš©í•©ë‹ˆë‹¤.
    (âš ï¸ ì£¼ì˜: í˜„ì¬ PoCì—ì„œëŠ” auto_applyëŠ” ë¬´ì‹œë©ë‹ˆë‹¤)
    """
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    try:
        import os
        dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
        
        # ê´€ë ¨ íŒŒì¼ ë‚´ìš© ìˆ˜ì§‘
        file_contents = {}
        if request.file_paths:
            for fp in request.file_paths[:10]:  # ìµœëŒ€ 10ê°œ íŒŒì¼
                if _validate_path(fp):
                    try:
                        content = await _read_file_content(request.workspace_id, fp)
                        file_contents[fp] = content
                    except:
                        pass
        
        if dev_mode:
            # ê°œë°œ ëª¨ë“œ: Mock ì‘ë‹µ ìƒì„±
            changes = _generate_mock_agent_changes(request.instruction, file_contents)
            return AIAgentResponse(
                summary=f"'{request.instruction}' ì‘ì—… ì™„ë£Œ (ê°œë°œ ëª¨ë“œ)",
                changes=changes,
                applied=False,
                tokensUsed=0,
            )
        
        # í”„ë¡œë•ì…˜ ëª¨ë“œ: ì‹¤ì œ LLM í˜¸ì¶œ
        try:
            llm_client = get_llm_client()
            
            # Agent í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            files_context = ""
            if file_contents:
                files_context = "\n\nFiles to modify:\n" + "\n".join([
                    f"### {fp}\n```\n{content}\n```" 
                    for fp, content in file_contents.items()
                ])
            
            messages = [
                {"role": "system", "content": """You are a coding agent. Given an instruction, analyze the code and provide specific changes.

Output format (JSON):
{
  "summary": "What was done",
  "changes": [
    {
      "filePath": "path/to/file.py",
      "action": "modify|create|delete",
      "diff": "--- a/file.py\\n+++ b/file.py\\n@@ -1,3 +1,4 @@\\n...",
      "description": "What this change does"
    }
  ]
}

Be specific and provide actual code changes in unified diff format."""},
                {"role": "user", "content": f"Instruction: {request.instruction}{files_context}"}
            ]
            
            llm_response = await llm_client.chat(messages=messages)
            response_text = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                import json
                if "```json" in response_text:
                    json_str = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    json_str = response_text.split("```")[1].split("```")[0]
                else:
                    json_str = response_text
                    
                agent_data = json.loads(json_str.strip())
                changes = [
                    FileChange(
                        filePath=c.get("filePath", "unknown"),
                        action=c.get("action", "modify"),
                        diff=c.get("diff"),
                        description=c.get("description", ""),
                    )
                    for c in agent_data.get("changes", [])
                ]
                summary = agent_data.get("summary", "Changes generated")
            except:
                changes = [FileChange(
                    filePath="unknown",
                    action="modify",
                    description=response_text[:500],
                )]
                summary = "Agent response (parsing failed)"
            
            return AIAgentResponse(
                summary=summary,
                changes=changes,
                applied=False,  # PoCì—ì„œëŠ” ìë™ ì ìš© ë¹„í™œì„±í™”
                tokensUsed=tokens_used,
            )
            
        except (LLMTimeoutError, LLMError) as e:
            return AIAgentResponse(
                summary=f"âš ï¸ LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}",
                changes=[],
                applied=False,
                tokensUsed=0,
            )
            
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR", "detail": str(e)},
        )


def _generate_mock_agent_changes(instruction: str, file_contents: dict) -> list:
    """ê°œë°œ ëª¨ë“œìš© Mock Agent ë³€ê²½ ì‚¬í•­ ìƒì„±"""
    changes = []
    
    if "ì£¼ì„" in instruction or "comment" in instruction.lower():
        for fp, content in file_contents.items():
            lines = content.split("\n")
            diff_lines = [f"--- a/{fp}", f"+++ b/{fp}", "@@ -1,3 +1,4 @@"]
            if lines:
                diff_lines.append(f"+# {instruction}")
                diff_lines.append(f" {lines[0]}")
            
            changes.append(FileChange(
                filePath=fp,
                action="modify",
                diff="\n".join(diff_lines),
                description=f"{fp}ì— ì£¼ì„ ì¶”ê°€",
            ))
    elif "ìƒì„±" in instruction or "create" in instruction.lower():
        changes.append(FileChange(
            filePath="new_file.py",
            action="create",
            diff="""--- /dev/null
+++ b/new_file.py
@@ -0,0 +1,5 @@
+\"\"\"
+Auto-generated file
+\"\"\"
+
+# TODO: Implement
""",
            description="ìƒˆ íŒŒì¼ ìƒì„±",
        ))
    else:
        # ê¸°ì¡´ íŒŒì¼ ìˆ˜ì • ì œì•ˆ
        for fp in list(file_contents.keys())[:2]:
            changes.append(FileChange(
                filePath=fp,
                action="modify",
                description=f"'{instruction}'ì— ë”°ë¼ {fp} ìˆ˜ì • í•„ìš” (ê°œë°œ ëª¨ë“œ)",
            ))
    
    if not changes:
        changes.append(FileChange(
            filePath="example.py",
            action="modify",
            description=f"'{instruction}' ì‘ì—…ì„ ìœ„í•œ ë³€ê²½ ì‚¬í•­ (ê°œë°œ ëª¨ë“œ)",
        ))
    
    return changes


# ============================================================
# AI Debug Mode - ë²„ê·¸ ë¶„ì„/ìˆ˜ì •
# ============================================================

@router.post(
    "/debug",
    response_model=AIDebugResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        403: {"model": ErrorResponse, "description": "Forbidden"},
        503: {"model": ErrorResponse, "description": "LLM service unavailable"},
    },
    summary="ë²„ê·¸ ë¶„ì„/ìˆ˜ì • (Debug ëª¨ë“œ)",
    description="ì—ëŸ¬ ë©”ì‹œì§€, ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤, ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ ë²„ê·¸ ì›ì¸ê³¼ í•´ê²°ì±…ì„ ì œì‹œí•©ë‹ˆë‹¤.",
)
async def debug_code(request: AIDebugRequest):
    """
    ë””ë²„ê·¸ ëª¨ë“œ: ì—ëŸ¬ ë©”ì‹œì§€, ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤, ê´€ë ¨ ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬
    ë²„ê·¸ì˜ ì›ì¸ì„ ì§„ë‹¨í•˜ê³  ìˆ˜ì • ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.
    """
    if not _validate_workspace_access(request.workspace_id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error": "Forbidden", "code": "WS_ACCESS_DENIED"},
        )
    
    if not request.error_message and not request.stack_trace and not request.description:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": "At least one of error_message, stack_trace, or description is required", "code": "DEBUG_NO_INPUT"},
        )
    
    try:
        import os
        dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
        
        # íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        file_content = None
        if request.file_path and _validate_path(request.file_path):
            try:
                if request.file_content:
                    file_content = request.file_content
                else:
                    file_content = await _read_file_content(request.workspace_id, request.file_path)
            except:
                pass
        
        if dev_mode:
            # ê°œë°œ ëª¨ë“œ: Mock ì§„ë‹¨ ìƒì„±
            diagnosis, root_cause, fixes, tips = _generate_mock_debug_response(
                error_message=request.error_message,
                stack_trace=request.stack_trace,
                file_path=request.file_path,
                file_content=file_content,
                description=request.description,
            )
            return AIDebugResponse(
                diagnosis=diagnosis,
                rootCause=root_cause,
                fixes=fixes,
                preventionTips=tips,
                tokensUsed=0,
            )
        
        # í”„ë¡œë•ì…˜ ëª¨ë“œ: ì‹¤ì œ LLM í˜¸ì¶œ
        try:
            llm_client = get_llm_client()
            
            # Debug í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            debug_context = []
            if request.error_message:
                debug_context.append(f"Error Message:\n{request.error_message}")
            if request.stack_trace:
                debug_context.append(f"Stack Trace:\n{request.stack_trace}")
            if request.description:
                debug_context.append(f"Description:\n{request.description}")
            if file_content:
                debug_context.append(f"Code ({request.file_path}):\n```\n{file_content}\n```")
            
            messages = [
                {"role": "system", "content": """You are a debugging expert. Analyze the error and provide a diagnosis.

Output format (JSON):
{
  "diagnosis": "Detailed analysis of the problem",
  "rootCause": "The root cause of the bug",
  "fixes": [
    {
      "filePath": "path/to/file.py",
      "lineNumber": 42,
      "originalCode": "old code",
      "fixedCode": "new code",
      "explanation": "Why this fixes the issue"
    }
  ],
  "preventionTips": ["Tip 1", "Tip 2"]
}

Be thorough and specific in your analysis."""},
                {"role": "user", "content": "\n\n".join(debug_context)}
            ]
            
            llm_response = await llm_client.chat(messages=messages)
            response_text = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens_used = llm_response.get("usage", {}).get("total_tokens", 0)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                import json
                if "```json" in response_text:
                    json_str = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    json_str = response_text.split("```")[1].split("```")[0]
                else:
                    json_str = response_text
                    
                debug_data = json.loads(json_str.strip())
                fixes = [
                    BugFix(
                        filePath=f.get("filePath", request.file_path or "unknown"),
                        lineNumber=f.get("lineNumber"),
                        originalCode=f.get("originalCode", ""),
                        fixedCode=f.get("fixedCode", ""),
                        explanation=f.get("explanation", ""),
                    )
                    for f in debug_data.get("fixes", [])
                ]
                diagnosis = debug_data.get("diagnosis", "Analysis complete")
                root_cause = debug_data.get("rootCause", "Unknown")
                tips = debug_data.get("preventionTips", [])
            except:
                diagnosis = response_text[:500]
                root_cause = "LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"
                fixes = []
                tips = []
            
            return AIDebugResponse(
                diagnosis=diagnosis,
                rootCause=root_cause,
                fixes=fixes,
                preventionTips=tips,
                tokensUsed=tokens_used,
            )
            
        except (LLMTimeoutError, LLMError) as e:
            return AIDebugResponse(
                diagnosis=f"âš ï¸ LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}",
                rootCause="LLM ì„œë¹„ìŠ¤ ì—°ê²° í•„ìš”",
                fixes=[],
                preventionTips=["vLLM ì„œë²„ ìƒíƒœ í™•ì¸", "VLLM_BASE_URL í™˜ê²½ë³€ìˆ˜ í™•ì¸"],
                tokensUsed=0,
            )
            
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "Internal server error", "code": "INTERNAL_ERROR", "detail": str(e)},
        )


def _generate_mock_debug_response(
    error_message: str | None,
    stack_trace: str | None,
    file_path: str | None,
    file_content: str | None,
    description: str | None,
) -> tuple:
    """ê°œë°œ ëª¨ë“œìš© Mock ë””ë²„ê·¸ ì‘ë‹µ ìƒì„±"""
    
    # ì—ëŸ¬ ìœ í˜• ë¶„ì„
    error_lower = (error_message or "").lower() + (stack_trace or "").lower()
    
    if "typeerror" in error_lower or "type" in error_lower:
        diagnosis = "TypeErrorê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë³€ìˆ˜ì˜ íƒ€ì…ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤."
        root_cause = "í•¨ìˆ˜ì— ì˜ëª»ëœ íƒ€ì…ì˜ ì¸ìê°€ ì „ë‹¬ë˜ì—ˆê±°ë‚˜, None ê°’ì— ëŒ€í•´ ë©”ì„œë“œë¥¼ í˜¸ì¶œí–ˆìŠµë‹ˆë‹¤."
        fixes = [BugFix(
            filePath=file_path or "unknown.py",
            lineNumber=1,
            originalCode="result = data.process()",
            fixedCode="result = data.process() if data is not None else None",
            explanation="None ì²´í¬ë¥¼ ì¶”ê°€í•˜ì—¬ TypeError ë°©ì§€",
        )]
        tips = ["íƒ€ì… íŒíŠ¸ ì‚¬ìš©", "None ì²´í¬ ì¶”ê°€", "isinstance()ë¡œ íƒ€ì… ê²€ì¦"]
        
    elif "importerror" in error_lower or "modulenotfound" in error_lower:
        diagnosis = "ëª¨ë“ˆ ì„í¬íŠ¸ ì˜¤ë¥˜ì…ë‹ˆë‹¤. í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤."
        root_cause = "íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜, ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        fixes = [BugFix(
            filePath="requirements.txt",
            lineNumber=None,
            originalCode="",
            fixedCode="missing_package>=1.0.0",
            explanation="í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ requirements.txtì— ì¶”ê°€",
        )]
        tips = ["pip install ì‹¤í–‰", "ê°€ìƒí™˜ê²½ í™•ì¸", "PYTHONPATH í™•ì¸"]
        
    elif "syntaxerror" in error_lower:
        diagnosis = "ë¬¸ë²• ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì½”ë“œì— êµ¬ë¬¸ ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤."
        root_cause = "ê´„í˜¸, ì½œë¡ , ë“¤ì—¬ì“°ê¸° ë“± Python ë¬¸ë²• ê·œì¹™ ìœ„ë°˜"
        fixes = [BugFix(
            filePath=file_path or "unknown.py",
            lineNumber=1,
            originalCode="def func(",
            fixedCode="def func():",
            explanation="í•¨ìˆ˜ ì •ì˜ ë¬¸ë²• ìˆ˜ì •",
        )]
        tips = ["IDEì˜ ë¦°í„° í™œì„±í™”", "ì½”ë“œ í¬ë§·í„° ì‚¬ìš©", "ê´„í˜¸ ì§ í™•ì¸"]
        
    else:
        diagnosis = f"""## ë””ë²„ê·¸ ë¶„ì„ (ê°œë°œ ëª¨ë“œ)

**ì—ëŸ¬**: {error_message or 'ì—†ìŒ'}

**ì„¤ëª…**: {description or 'ì—†ìŒ'}

{'**ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤**:' + chr(10) + '```' + chr(10) + stack_trace[:500] + chr(10) + '```' if stack_trace else ''}

ì´ ë¶„ì„ì€ ê°œë°œ ëª¨ë“œì—ì„œ ìƒì„±ëœ Mock ì‘ë‹µì…ë‹ˆë‹¤.
ì‹¤ì œ AI ë¶„ì„ì„ ìœ„í•´ vLLM ì„œë²„ë¥¼ ì—°ê²°í•´ì£¼ì„¸ìš”."""
        root_cause = "ê°œë°œ ëª¨ë“œì—ì„œëŠ” ì •í™•í•œ ì›ì¸ ë¶„ì„ì´ ì œí•œë©ë‹ˆë‹¤."
        fixes = []
        tips = ["vLLM ì„œë²„ ì—°ê²° í›„ ì¬ì‹œë„", "ì—ëŸ¬ ë¡œê·¸ ìì„¸íˆ í™•ì¸", "ê´€ë ¨ ì½”ë“œ ê²€í† "]
    
    return diagnosis, root_cause, fixes, tips


# ============================================================
# AI Mode Status
# ============================================================

@router.get(
    "/modes",
    summary="ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë“œ ëª©ë¡",
    description="í˜„ì¬ ì§€ì›í•˜ëŠ” AI ëª¨ë“œ ëª©ë¡ê³¼ ì„¤ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
)
async def get_available_modes():
    """ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë“œ ëª©ë¡ ë°˜í™˜"""
    return {
        "modes": [
            {
                "id": "ask",
                "name": "Ask",
                "description": "ì½”ë“œì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  ë‹µë³€ì„ ë°›ìŠµë‹ˆë‹¤.",
                "icon": "ğŸ’¬",
            },
            {
                "id": "agent",
                "name": "Agent",
                "description": "ìë™ìœ¼ë¡œ ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ë³€ê²½ ì‚¬í•­ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
                "icon": "ğŸ¤–",
            },
            {
                "id": "plan",
                "name": "Plan",
                "description": "ëª©í‘œë¥¼ ë¶„ì„í•˜ê³  ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤.",
                "icon": "ğŸ“‹",
            },
            {
                "id": "debug",
                "name": "Debug",
                "description": "ì—ëŸ¬ë¥¼ ë¶„ì„í•˜ê³  ë²„ê·¸ ìˆ˜ì • ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.",
                "icon": "ğŸ›",
            },
        ],
        "current": "ask",  # ê¸°ë³¸ ëª¨ë“œ
    }
